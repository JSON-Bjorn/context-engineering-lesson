{
  "documents": [
    {
      "id": "doc_001",
      "title": "Introduction to Context Windows",
      "content": "A context window is the maximum amount of text, measured in tokens, that a Large Language Model (LLM) can process in a single request. This fundamental constraint shapes how we interact with and utilize these powerful AI systems.\n\nThe context window encompasses everything: the system instructions, the user's prompt, any retrieved documents or background information, the conversation history in multi-turn dialogues, and the space reserved for the model's response. Understanding this limitation is crucial for effective prompt engineering and system design.\n\nHistorically, context windows have grown dramatically. Early GPT-3 models offered 4,096 tokens (roughly 3,000 words). GPT-4 expanded this to 8,192 tokens in its initial release, then 32,768 in extended versions. Claude 2 pushed boundaries with 100,000 tokens, and current models like Claude 3.5 Sonnet offer up to 200,000 tokens.\n\nThis expansion enables new applications: processing entire books, analyzing lengthy codebases, maintaining extended conversations, and retrieving information from large document collections. However, larger context windows also present challenges: increased cost per request, slower inference times, and the phenomenon known as 'lost in the middle' where models struggle to attend to information buried deep within long contexts.\n\nEffective context engineering requires understanding these trade-offs and strategically managing what information gets included, where it's positioned, and how it's formatted within the available window.",
      "tokens": 267,
      "category": "fundamentals",
      "relevance_keywords": ["context window", "tokens", "limitations", "history", "GPT", "Claude"]
    },
    {
      "id": "doc_002",
      "title": "Token Counting and Budget Management",
      "content": "Tokens are the fundamental units that LLMs process. Unlike simple character or word counts, tokenization follows specific rules that vary by model family. Understanding tokenization is essential for accurate context budgeting.\n\nMost modern LLMs use subword tokenization schemes. GPT models employ Byte Pair Encoding (BPE) with the cl100k_base encoding, where common words might be single tokens ('the', 'and') while uncommon words or technical terms split into multiple tokens. For example, 'context' is one token, but 'contextualization' might be two or three.\n\nPractical token estimation rules: English text averages about 0.75 tokens per word, or approximately 1,300-1,500 words per 1,000 tokens. Code is generally more token-dense than prose, with Python averaging around 0.5 words per token. Special characters, whitespace, and formatting all consume tokens.\n\nTools for token counting include tiktoken (Python library for OpenAI encodings), official model APIs with token count endpoints, and online calculators. However, always measure with the actual model's tokenizer for production systems, as estimates can vary by 10-20%.\n\nBudget management strategies: Always reserve overhead for system instructions, the user's question, and the model's response - typically 200-400 tokens. Monitor token consumption in real-time as you assemble context. Implement truncation strategies for when content exceeds limits: hard cutoffs at token boundaries, intelligent summarization of less critical sections, or dynamic allocation based on relevance scores.\n\nCost considerations matter too. Cloud-based LLM APIs charge per token, with input and output tokens sometimes priced differently. A single request with 10,000 input tokens and 500 output tokens to GPT-4 might cost $0.30-0.50. Optimizing token usage directly impacts operational costs at scale.",
      "tokens": 354,
      "category": "fundamentals",
      "relevance_keywords": ["tokens", "tokenization", "BPE", "counting", "budget", "cost"]
    },
    {
      "id": "doc_003",
      "title": "The Lost in the Middle Phenomenon",
      "content": "Research by Liu et al. (2023) revealed a critical flaw in how LLMs process long contexts: information positioned in the middle of long contexts is significantly less likely to be recalled and utilized than information at the beginning or end. This 'lost in the middle' effect has profound implications for context engineering.\n\nThe researchers tested models across various context lengths and found consistent U-shaped performance curves. When relevant information appeared in the first 10% or last 10% of the context, models achieved 80-90% accuracy. However, when the same information appeared in the middle 50% of the context, accuracy dropped to 40-60%.\n\nThis phenomenon persists across model families and sizes. Neither GPT-4, Claude, nor open-source alternatives like Llama-2 escape this limitation. Even models explicitly trained on long contexts show degraded middle-attention, though recent architectural improvements have reduced the severity.\n\nWhy does this happen? Current transformer architectures use attention mechanisms that create implicit positional biases. The self-attention computation naturally weights information near the query position (the end) more heavily. Additionally, during training, models see more examples where critical information appears at context boundaries - the start of documents or near the conclusion - creating learned biases.\n\nPractical implications: When designing RAG systems, place the most relevant retrieved documents at the beginning or end of the context, not the middle. For multi-document question answering, use a 'sandwich' strategy: put high-priority documents at both ends, with lower-priority content in the middle. In conversational systems with long chat histories, consider summarizing or removing middle turns while preserving recent exchanges and important early context.\n\nMitigation strategies include: breaking long contexts into multiple shorter contexts with independent queries, using explicit instructions to direct attention ('Pay special attention to Document 5'), implementing retrieval re-ranking to ensure critical information lands at context boundaries, and employing attention-highlighting techniques during inference when available.",
      "tokens": 384,
      "category": "fundamentals",
      "relevance_keywords": ["lost in the middle", "attention", "position bias", "research", "U-shaped curve"]
    },
    {
      "id": "doc_004",
      "title": "Context Placement Strategies: Primacy vs Recency",
      "content": "The order in which you present information within a context window significantly impacts model performance. Two fundamental strategies compete: primacy (placing important information first) and recency (placing it last).\n\nPrimacy placement leverages the strong attention models give to early context. System instructions traditionally appear at the start for this reason. When you place critical documents first, you ensure they receive thorough processing before the model's attention capacity is taxed by additional content. This approach works particularly well for instructional content, definitions, and foundational concepts that inform later reasoning.\n\nAdvantages of primacy: establishes context early, benefits from fresh attention capacity, aligns with human reading patterns, and works reliably across all model families. Disadvantages: later information might override early content, and models may forget early details when generating responses to questions about recent context.\n\nRecency placement positions critical information immediately before the query. This capitalizes on proximity bias - information nearest to the generation point receives heightened attention. Models show stronger recall for recently processed content, especially in question-answering tasks.\n\nAdvantages of recency: maximizes immediate availability for response generation, reduces risk of forgetting over long contexts, works exceptionally well for factual retrieval, and proves more robust as context length increases. Disadvantages: may feel unintuitive, requires careful prompt engineering to avoid confusion, and can struggle when responses need to synthesize across the entire context.\n\nEmpirical results from controlled experiments show recency typically outperforms primacy by 5-15% on factual retrieval tasks with contexts over 4,000 tokens. However, primacy maintains advantages for tasks requiring sustained reasoning or when the model must integrate information throughout its response.\n\nBest practices: Use primacy for instructions, schemas, and conceptual frameworks. Use recency for facts, specific details, and retrieved documents in RAG systems. For complex tasks requiring both, implement a sandwich strategy combining both approaches.",
      "tokens": 368,
      "category": "strategies",
      "relevance_keywords": ["primacy", "recency", "placement", "order", "attention bias"]
    },
    {
      "id": "doc_005",
      "title": "The Sandwich Strategy: Optimal Context Organization",
      "content": "The sandwich strategy represents the current best practice for organizing information in long contexts. By placing high-priority content at both the beginning and end of the context, with lower-priority material in the middle, you maximize model performance while efficiently utilizing available tokens.\n\nImplementation details: Rank all available documents or information chunks by relevance to the query. Select the top 20-40% as high-priority content. Split this high-priority group into two subgroups. Place the first subgroup at the very start of the context, immediately after system instructions. Insert lower-priority content in the middle section. Place the second high-priority subgroup at the end, just before the user's question.\n\nThis approach exploits both primacy and recency effects while deliberately sacrificing attention to middle content, where the model naturally performs worse anyway. By ensuring that less-critical information occupies the middle zone, you minimize the impact of the 'lost in the middle' phenomenon.\n\nEmpirical results: In benchmark tests across multiple question-answering datasets, the sandwich strategy achieves 15-25% higher accuracy than naive concatenation, 8-12% better than pure primacy, and 3-7% better than pure recency. These gains hold across different context lengths from 4K to 32K tokens.\n\nAdvanced variations: Weighted sandwiching allocates tokens proportionally to relevance scores, ensuring higher-priority content gets more space. Multi-layer sandwiching creates nested importance hierarchies with the most critical content at the absolute boundaries, secondary content in the outer-middle, and tertiary content in the center core. Dynamic sandwiching adjusts the split ratio based on query type - factual queries favor heavier recency weighting (30/40/30 split), while analytical queries favor balanced weighting (40/20/40 split).\n\nImplementation challenges: Requires relevance scoring mechanism, adds computational overhead for document ranking, may disrupt narrative flow in conversational contexts, and needs careful tuning of split ratios for optimal performance. Despite these challenges, the performance gains make sandwiching the recommended default for most production systems handling contexts over 2,000 tokens.",
      "tokens": 394,
      "category": "strategies",
      "relevance_keywords": ["sandwich", "strategy", "optimization", "high-priority", "split"]
    },
    {
      "id": "doc_006",
      "title": "Retrieval-Augmented Generation Context Assembly",
      "content": "Retrieval-Augmented Generation (RAG) systems face unique context engineering challenges. They must combine retrieved documents with user queries in a way that maximizes answer quality while respecting token limits.\n\nTypical RAG architecture: A user submits a query. The system generates an embedding for the query. This embedding searches a vector database of pre-chunked and embedded documents. The top-k most similar chunks are retrieved (commonly k=5-20). These chunks, along with the original query, form the context for the LLM. The LLM generates a response based on this assembled context.\n\nContext assembly challenges in RAG: Retrieved chunks may be redundant, containing duplicate information. Chunks may lack necessary surrounding context from their source documents. The optimal ordering of retrieved chunks is unclear. Token budgets may not accommodate all retrieved chunks. Chunk boundaries may split important information.\n\nBest practices for RAG context assembly: Use semantic chunking that respects document structure (paragraphs, sections) rather than fixed-size splits. Implement relevance re-ranking after initial retrieval using a cross-encoder model for more accurate similarity scoring. Apply deduplication to remove highly similar chunks before assembly. Include metadata with each chunk (source document title, date, author) to aid model understanding. Use the sandwich strategy for chunk ordering: most relevant chunks at start and end.\n\nAdvanced techniques: Parent-child chunking maintains both fine-grained chunks for retrieval and their parent sections for context. Retrieve small chunks but provide larger parent contexts to the LLM. Hypothetical document embeddings (HyDE) generate hypothetical answers to the query, embed them, and use these embeddings for retrieval rather than the query itself. Multi-hop retrieval performs iterative searches, using information from initial results to refine subsequent queries. Query decomposition breaks complex queries into simpler sub-queries, retrieves for each, then synthesizes results.\n\nContext compression methods: Summarize less-relevant chunks while keeping high-relevance chunks in full. Extract key facts from chunks rather than including complete text. Use LLM-based filtering to remove irrelevant sentences before including chunks. Dynamic token allocation assigns more tokens to higher-relevance chunks.\n\nTesting and evaluation: Measure end-to-end accuracy with and without retrieval. Track answer attribution - can the model cite specific chunks? Monitor context utilization - which chunks are actually used? A/B test different assembly strategies in production.",
      "tokens": 456,
      "category": "optimization",
      "relevance_keywords": ["RAG", "retrieval", "chunks", "vector database", "assembly"]
    },
    {
      "id": "doc_007",
      "title": "Hierarchical Summarization for Context Compression",
      "content": "When facing severe token constraints, hierarchical summarization offers a powerful technique for compressing large amounts of information while preserving critical content.\n\nCore concept: Instead of including all documents in full text, selectively summarize less-relevant documents while keeping high-relevance documents complete. This allows broader coverage of available information within a fixed token budget.\n\nImplementation approach: Rank all candidate documents by relevance score using semantic similarity to the query. Define a threshold (e.g., top 30% of documents) for full inclusion. Documents above this threshold are included in their entirety. Documents below the threshold are passed through a summarization process. Summarization can use extractive methods (selecting key sentences) or abstractive methods (LLM-generated summaries). Summaries typically target 20-30% of original length.\n\nMulti-level hierarchies: For very large document sets, implement multiple summarization tiers. Tier 1 (highest relevance): full documents, 100% of content. Tier 2 (medium relevance): detailed summaries, 40-50% of content. Tier 3 (low relevance): brief summaries, 15-25% of content. Tier 4 (marginal relevance): title and single-sentence description only.\n\nSummarization quality is critical. Poor summaries can omit key facts or introduce errors. Best practices: Use reliable summarization models (GPT-3.5-turbo or Claude Haiku are cost-effective choices). Provide clear instructions: 'Summarize the following document in 3-4 sentences, focusing on information relevant to [query topic]'. Validate summaries against originals in testing. Consider ensemble approaches using multiple summarization methods.\n\nPerformance characteristics: Hierarchical summarization typically increases coverage by 2-3x (including information from 20 documents instead of 7 within the same token budget). Accuracy impact varies by use case. For questions whose answers appear in high-relevance documents: minimal accuracy loss (0-5%). For questions requiring synthesis across many documents: potential accuracy gain (10-20%) due to broader coverage. For questions whose answers are in low-relevance documents: potential accuracy loss (15-30%) due to summarization dropping critical details.\n\nCost considerations: Generating summaries adds API costs and latency. Pre-compute and cache summaries when possible for frequently-accessed documents. Use faster, cheaper models for summarization (quality loss is often acceptable for lower-relevance content). Consider offline batch processing for large document sets.\n\nCombining with other strategies: Hierarchical summarization pairs well with sandwich placement. Put full-text high-relevance documents at context boundaries, summaries in the middle. Combine with semantic chunking for finer control. Use dynamic token allocation to adjust summary lengths based on available budget.",
      "tokens": 502,
      "category": "optimization",
      "relevance_keywords": ["summarization", "compression", "hierarchy", "tiers", "cost"]
    },
    {
      "id": "doc_008",
      "title": "Memory Management in Multi-Turn Conversations",
      "content": "Multi-turn conversations present unique context engineering challenges. Each exchange adds to the context, eventually exceeding token limits. Effective memory management is essential for maintaining coherent, long-running interactions.\n\nThe accumulation problem: A typical exchange consumes 100-300 tokens (user message + assistant response). After 20 turns, context reaches 2,000-6,000 tokens. After 50 turns, you exceed most context windows. Simply truncating old messages loses important context and breaks conversation continuity.\n\nMemory management strategies: Sliding window maintains only the most recent N turns, discarding older messages. Simple but risks losing important context from early conversation. Summarization periodically compresses old conversation history into a summary, maintaining recent turns in full. Balances context preservation with token efficiency. Selective retention identifies and preserves important messages (user preferences, key facts, decisions) while discarding routine exchanges. Requires message importance scoring. Hierarchical memory maintains summaries at multiple time scales: recent turns in full, medium-past turns summarized, distant turns compressed to key points.\n\nImplementation details for summarization: Trigger summarization when context reaches 70-80% of limit. Summarize all but the most recent 5-10 turns. Prompt the LLM: 'Summarize the following conversation history, preserving key facts, user preferences, and important context. Format as a concise summary paragraph.' Store this summary at the beginning of the context. As conversation continues, update the summary periodically.\n\nSelective retention scoring: Assign importance scores to each message based on criteria including presence of user preferences or personal information, factual statements or decisions, emotional content or sentiment shifts, explicit memory requests ('remember that...'), and references to future actions. Retain high-scoring messages in full, summarize medium-scoring messages, and discard low-scoring routine exchanges.\n\nContext reconstruction: When a user references something from early in the conversation, the system may need to reconstruct that context. Maintain a searchable archive of full conversation history outside the active context. When the model indicates uncertainty ('I don't recall...') or the user explicitly references past exchanges, search the archive for relevant turns and temporarily inject them into context.\n\nUser control: Provide mechanisms for users to influence memory management including ability to pin important messages that should never be summarized or removed, explicit memory commands ('forget about X', 'remember that Y'), and summary review to allow users to edit or amend conversation summaries.\n\nTesting strategies: Measure conversation coherence over long interactions. Track reference resolution - can the model correctly resolve pronouns and references? Monitor recall accuracy for facts mentioned earlier. Evaluate user satisfaction with memory management through explicit feedback.",
      "tokens": 529,
      "category": "optimization",
      "relevance_keywords": ["memory", "conversation", "multi-turn", "sliding window", "history"]
    },
    {
      "id": "doc_009",
      "title": "Case Study: Customer Support Context Engineering",
      "content": "A major e-commerce company implemented a customer support chatbot using context engineering best practices. This case study illustrates real-world application of context optimization techniques.\n\nInitial system (baseline): The bot retrieved the top 10 most similar support articles based on customer query embedding similarity. Articles were concatenated in relevance order and sent to GPT-4 with an 8K context window. Average token usage: 6,500 per query. Answer accuracy (measured by customer satisfaction and agent review): 67%. Cost per query: $0.18.\n\nProblem identified: The middle-positioned articles (ranks 4-7) were rarely referenced in responses despite often containing relevant information. Agent reviews noted the bot sometimes missed relevant solutions that appeared in the retrieved articles but were 'lost in the middle.'\n\nPhase 1 optimization - Sandwich placement: Implemented sandwich strategy with top 4 articles split: ranks 1-2 at the start, ranks 3-4 at the end, ranks 5-10 in the middle. Average token usage: 6,500 (unchanged). Answer accuracy: 73% (+6 percentage points). Cost per query: $0.18 (unchanged). Implementation time: 2 days.\n\nPhase 2 optimization - Hierarchical summarization: Full text for top 4 articles (ranks 1-4). Summarized lower-relevance articles (ranks 5-10) to 30% of original length using GPT-3.5-turbo. Average token usage: 4,200 (-35%). Answer accuracy: 76% (+9 percentage points overall). Cost per query: $0.13 (-28%). Implementation time: 1 week including summary caching infrastructure.\n\nPhase 3 optimization - Dynamic allocation: Implemented token budget allocation proportional to relevance scores. Highest-relevance articles could use up to 1,500 tokens each, medium-relevance up to 800 tokens, low-relevance up to 400 tokens. Added truncation logic that preserved article beginning and end while cutting middle content if needed. Average token usage: 4,500 (+7% from Phase 2 but still -31% from baseline). Answer accuracy: 79% (+12 percentage points overall). Cost per query: $0.14. Implementation time: 2 weeks.\n\nPhase 4 optimization - User context integration: Added customer order history, previous support tickets, and account information to context using the sandwich strategy. This information used high-priority placement (start of context). Support articles filled the middle. Average token usage: 5,200. Answer accuracy: 84% (+17 percentage points overall). Customer satisfaction score: increased from 3.2/5.0 to 4.1/5.0. Cost per query: $0.16. Implementation time: 3 weeks including privacy/security review.\n\nFinal results: 17 percentage point improvement in accuracy (67% to 84%). 11% cost reduction despite adding more features. 28% improvement in customer satisfaction. 35% reduction in escalations to human agents. Payback period: 3 months. Key learnings: Context placement matters more than retrieval quality alone. Hierarchical approaches enable better information coverage. Domain-specific context (customer history) provides substantial value. Iterative optimization compounds gains.",
      "tokens": 626,
      "category": "case_studies",
      "relevance_keywords": ["case study", "customer support", "optimization", "results", "ROI"]
    },
    {
      "id": "doc_010",
      "title": "Case Study: Document Q&A System Optimization",
      "content": "A legal tech startup built a document Q&A system for analyzing contracts. Their journey through context engineering challenges illustrates advanced optimization techniques.\n\nSystem requirements: Handle contracts ranging from 5,000 to 50,000 tokens. Answer complex questions requiring multi-clause reasoning. Maintain 95%+ accuracy for clause extraction. Process queries in under 10 seconds. Support batch processing of hundreds of documents.\n\nInitial approach: Chunk contracts into 500-token segments with 50-token overlap. Embed all chunks using OpenAI ada-002. For each query, retrieve top 10 most similar chunks. Concatenate chunks and query GPT-4. Results: Accuracy 78% (below requirement). Average query time: 8 seconds. Problem: Relevant information often split across multiple non-contiguous chunks. Critical context from surrounding clauses frequently missed.\n\nOptimization 1 - Parent-child chunking: Created two chunk hierarchies. Child chunks: 200 tokens, used for retrieval matching. Parent chunks: 800 tokens (original child plus surrounding context). Retrieved based on child chunk similarity but included parent chunks in context for the LLM. Results: Accuracy 85% (+7 percentage points). Query time: 9 seconds. Improvement: Better preservation of contextual information around matched segments.\n\nOptimization 2 - Query decomposition: For complex questions, used GPT-3.5 to decompose into sub-questions. Retrieve independently for each sub-question. Assembled context with sub-question-specific chunks interleaved. Final LLM call synthesized across all retrieved information. Results: Accuracy 89% (+4 percentage points). Query time: 12 seconds (slower due to multiple retrievals). Best for: Complex questions requiring information from multiple contract sections.\n\nOptimization 3 - Semantic section detection: Used LLM to identify semantic sections in contracts (e.g., 'Payment Terms', 'Liability Clauses', 'Termination Conditions'). Created section-level embeddings in addition to chunk embeddings. For each query, first identified relevant sections, then retrieved chunks within those sections. Used hierarchical context assembly: section summaries at high level, detailed chunks for most relevant sections. Results: Accuracy 92% (+3 percentage points). Query time: 10 seconds (back under target through focused retrieval). Breakthrough: Section-aware retrieval dramatically reduced irrelevant chunks in context.\n\nOptimization 4 - Attention-guided refinement: Two-pass approach. Pass 1: Standard retrieval and initial answer generation. Pass 2: LLM identifies which parts of the context were most useful for the answer. System retrieves additional chunks related to those high-attention segments. Re-generates answer with expanded context. Results: Accuracy 96% (+4 percentage points, exceeding target). Query time: 15 seconds (acceptable for high-value queries). Best for: Critical queries where accuracy matters more than speed.\n\nFinal system architecture: Hybrid approach using different strategies based on query characteristics. Simple factual queries: Basic parent-child chunking with sandwich placement. Complex analytical queries: Query decomposition with section-aware retrieval. Critical accuracy-required queries: Attention-guided refinement. System automatically classifies queries and routes to appropriate strategy. Overall accuracy: 94% (weighted average across query types). Average query time: 11 seconds. Customer outcome: 10x increase in contract review throughput. 60% reduction in legal review costs. System adoption rate: 87% of legal team.",
      "tokens": 643,
      "category": "case_studies",
      "relevance_keywords": ["document Q&A", "legal", "contracts", "chunking", "multi-pass"]
    }
  ],
  "metadata": {
    "total_documents": 10,
    "total_tokens": 4523,
    "categories": ["fundamentals", "strategies", "optimization", "case_studies"],
    "category_distribution": {
      "fundamentals": 3,
      "strategies": 2,
      "optimization": 3,
      "case_studies": 2
    },
    "avg_tokens_per_document": 452,
    "min_tokens": 267,
    "max_tokens": 643,
    "created": "2025-10-19",
    "purpose": "Context engineering educational corpus for interactive lesson",
    "license": "Apache 2.0",
    "author": "[Your Name]"
  }
}
