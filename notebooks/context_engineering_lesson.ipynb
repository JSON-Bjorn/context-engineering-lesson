{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ Context Engineering: Optimizing LLM Context Windows\n",
    "\n",
    "Welcome to this interactive lesson! You'll learn how to strategically structure and optimize context for Large Language Models.\n",
    "\n",
    "**Duration:** ~30 minutes  \n",
    "**Difficulty:** Intermediate  \n",
    "**Approach:** 100% local, no API costs\n",
    "\n",
    "---\n",
    "\n",
    "## What You'll Build\n",
    "\n",
    "By the end of this lesson, you will:\n",
    "- ‚úÖ Understand token budgets and context constraints\n",
    "- ‚úÖ Implement 4 different context assembly strategies\n",
    "- ‚úÖ Measure and compare their performance quantitatively\n",
    "- ‚úÖ Apply optimization techniques to improve quality or reduce costs\n",
    "\n",
    "**Let's get started!** üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Import required libraries\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "# Import lesson modules\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from token_manager import count_tokens, fits_in_budget, TokenBudgetManager\n",
    "from helpers import load_documents, load_questions, calculate_similarity\n",
    "from evaluation import evaluate_answer, LLMEvaluator\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")\n",
    "print(f\"üìä PyTorch version: {torch.__version__}\")\n",
    "print(f\"üñ•Ô∏è  Device available: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Load models (this will download on first run)\n",
    "print(\"Loading models... (first run downloads ~6.6 GB, please be patient)\")\n",
    "print(\"Subsequent runs will be instant.\\n\")\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load LLM for generation\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "print(\"‚úÖ LLM loaded!\")\n",
    "\n",
    "# Load embedding model for similarity\n",
    "EMBED_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "print(f\"\\nLoading {EMBED_MODEL}...\")\n",
    "embedder = SentenceTransformer(EMBED_MODEL)\n",
    "print(\"‚úÖ Embedding model loaded!\")\n",
    "\n",
    "print(\"\\nüéâ All models ready! Let's start learning.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Load lesson data\n",
    "print(\"Loading lesson data...\")\n",
    "\n",
    "# Load documents\n",
    "documents = load_documents('../data/source_documents.json')\n",
    "print(f\"‚úÖ Loaded {len(documents)} documents\")\n",
    "\n",
    "# Load evaluation questions\n",
    "questions = load_questions('../data/evaluation_questions.json')\n",
    "print(f\"‚úÖ Loaded {len(questions)} evaluation questions\")\n",
    "\n",
    "# Preview first document\n",
    "print(\"\\nüìÑ Sample Document:\")\n",
    "print(f\"Title: {documents[0]['title']}\")\n",
    "print(f\"Tokens: {documents[0]['tokens']}\")\n",
    "print(f\"Preview: {documents[0]['content'][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìã Lesson Roadmap\n",
    "\n",
    "### Phase 1: Understanding Context Windows (7 min)\n",
    "Learn about token limits and budget constraints\n",
    "\n",
    "### Phase 2: Baseline Implementation (8 min)\n",
    "Build a naive context assembly function\n",
    "\n",
    "### Phase 3: Strategic Placement (10 min)\n",
    "Implement and compare three placement strategies:\n",
    "- **Primacy:** Important info at the start\n",
    "- **Recency:** Important info at the end\n",
    "- **Sandwich:** Important info at both ends\n",
    "\n",
    "### Phase 4: Optimization (5 min)\n",
    "Choose and implement one advanced optimization\n",
    "\n",
    "### Phase 5: Results & Evaluation\n",
    "Compare all strategies and see your improvements!\n",
    "\n",
    "---\n",
    "\n",
    "**Ready? Let's dive into Phase 1!** ‚¨áÔ∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1: Understanding Context Windows (7 minutes)\n",
    "\n",
    "## What is a Context Window?\n",
    "\n",
    "A **context window** is the maximum amount of text (measured in tokens) that an LLM can process at once. This includes:\n",
    "- Your prompt/instructions\n",
    "- Any retrieved documents or context\n",
    "- The user's question\n",
    "- The model's response\n",
    "\n",
    "## Why Does This Matter?\n",
    "\n",
    "Every token costs:\n",
    "- **Money:** API providers charge per token\n",
    "- **Time:** More tokens = slower inference\n",
    "- **Attention:** Models struggle with very long contexts (\"lost in the middle\")\n",
    "\n",
    "## Your Challenge\n",
    "\n",
    "You have 10 documents and need to answer questions about them. But they don't all fit in the context window at once!\n",
    "\n",
    "**Let's see what we're working with...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Analyze token counts\n",
    "print(\"üìä Document Token Analysis\\n\")\n",
    "\n",
    "# Calculate statistics\n",
    "total_tokens = sum(doc['tokens'] for doc in documents)\n",
    "avg_tokens = total_tokens / len(documents)\n",
    "min_tokens = min(doc['tokens'] for doc in documents)\n",
    "max_tokens = max(doc['tokens'] for doc in documents)\n",
    "\n",
    "print(f\"Total tokens across all documents: {total_tokens:,}\")\n",
    "print(f\"Average tokens per document: {avg_tokens:.0f}\")\n",
    "print(f\"Smallest document: {min_tokens} tokens\")\n",
    "print(f\"Largest document: {max_tokens} tokens\")\n",
    "\n",
    "# Visualize distribution\n",
    "token_counts = [doc['tokens'] for doc in documents]\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(range(len(token_counts)), token_counts, color='steelblue', alpha=0.7)\n",
    "plt.xlabel('Document Index')\n",
    "plt.ylabel('Token Count')\n",
    "plt.title('Token Distribution Across Documents')\n",
    "plt.axhline(y=avg_tokens, color='r', linestyle='--', label=f'Average ({avg_tokens:.0f})')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüí° Key Insight: All documents together = {total_tokens:,} tokens\")\n",
    "print(\"    Most LLMs have 4K-8K token windows. We need to be selective!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßÆ Exercise: What Fits in Different Windows?\n",
    "\n",
    "Given our total of ~8,500 tokens across all documents, let's see what fits in common context window sizes.\n",
    "\n",
    "Remember: You also need room for:\n",
    "- The question (~50 tokens)\n",
    "- The response (~200 tokens)\n",
    "- System instructions (~50 tokens)\n",
    "\n",
    "So subtract ~300 tokens from each window for overhead!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: TODO - Calculate what fits in different windows\n",
    "# This is your first coding task!\n",
    "\n",
    "def calculate_fit_analysis(documents, window_sizes=[2048, 4096, 8192]):\n",
    "    \"\"\"\n",
    "    TODO: For each window size, determine:\n",
    "    1. How many documents fit (accounting for 300 token overhead)\n",
    "    2. What percentage of total tokens can be included\n",
    "    3. Which specific documents fit (in order, until limit reached)\n",
    "    \n",
    "    Args:\n",
    "        documents: List of document dicts with 'tokens' field\n",
    "        window_sizes: List of context window sizes to analyze\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with results for each window size\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    overhead = 300  # tokens for question + response + instructions\n",
    "    \n",
    "    # TODO: Your implementation here\n",
    "    # Hint: Iterate through documents in order, track cumulative tokens\n",
    "    # Hint: Stop when adding next doc would exceed (window_size - overhead)\n",
    "    \n",
    "    for window_size in window_sizes:\n",
    "        available_tokens = window_size - overhead\n",
    "        # TODO: Calculate how many docs fit\n",
    "        # TODO: Calculate percentage of total\n",
    "        # TODO: Track which specific docs\n",
    "        \n",
    "        results[window_size] = {\n",
    "            'docs_fit': 0,  # TODO: Replace with actual count\n",
    "            'tokens_used': 0,  # TODO: Replace with actual sum\n",
    "            'percentage': 0,  # TODO: Replace with actual percentage\n",
    "            'doc_indices': []  # TODO: Replace with actual indices\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test your implementation\n",
    "fit_analysis = calculate_fit_analysis(documents)\n",
    "\n",
    "# Display results\n",
    "print(\"üìä Context Window Fit Analysis\\n\")\n",
    "for window_size, stats in fit_analysis.items():\n",
    "    print(f\"Window Size: {window_size:,} tokens\")\n",
    "    print(f\"  Documents that fit: {stats['docs_fit']}/{len(documents)}\")\n",
    "    print(f\"  Tokens used: {stats['tokens_used']:,}\")\n",
    "    print(f\"  Coverage: {stats['percentage']:.1f}%\")\n",
    "    print()\n",
    "\n",
    "# üí° HINT: If you're stuck, uncomment the next line\n",
    "# %load ../src/hints/hint_calculate_fit.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Phase 1 Complete!\n",
    "\n",
    "**What you learned:**\n",
    "- Context windows have hard token limits\n",
    "- Not all information can fit at once\n",
    "- Strategic selection is crucial\n",
    "\n",
    "**Key Takeaway:** With an 8K window, you can only fit ~75% of documents. **Which ones should you choose? And where should you put them?**\n",
    "\n",
    "That's what we'll explore next! ‚¨áÔ∏è\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2: Baseline Context Assembly (8 minutes)\n",
    "\n",
    "## The Naive Approach\n",
    "\n",
    "The simplest strategy: concatenate documents in order until you run out of space.\n",
    "\n",
    "**No intelligence, no optimization, just raw concatenation.**\n",
    "\n",
    "This will be our **baseline** for comparison. Every other strategy must beat this!\n",
    "\n",
    "## Your Task\n",
    "\n",
    "Implement `naive_context_assembly()` that:\n",
    "1. Takes documents and a query\n",
    "2. Concatenates documents in order\n",
    "3. Stops when approaching the token limit\n",
    "4. Returns the assembled context string\n",
    "\n",
    "Let's build it! üí™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: TODO - Implement naive context assembly\n",
    "\n",
    "def naive_context_assembly(documents, query, token_limit=4000):\n",
    "    \"\"\"\n",
    "    Naive context assembly: concatenate documents in order until token limit.\n",
    "    \n",
    "    Args:\n",
    "        documents: List of document dicts with 'content' and 'tokens' fields\n",
    "        query: The question being asked (string)\n",
    "        token_limit: Maximum tokens for context (int)\n",
    "    \n",
    "    Returns:\n",
    "        Assembled context string\n",
    "    \"\"\"\n",
    "    # TODO: Implement naive concatenation\n",
    "    # Hint 1: Reserve some tokens for the query itself (~50)\n",
    "    # Hint 2: Iterate through documents in order\n",
    "    # Hint 3: Keep track of cumulative tokens\n",
    "    # Hint 4: Stop when adding next doc would exceed limit\n",
    "    # Hint 5: Format nicely with document separators\n",
    "    \n",
    "    context_parts = []\n",
    "    used_tokens = 0\n",
    "    available_tokens = token_limit - 50  # Reserve for query\n",
    "    \n",
    "    # TODO: Your implementation here\n",
    "    \n",
    "    return \"\\n\\n\".join(context_parts)\n",
    "\n",
    "# Test your implementation\n",
    "test_query = questions[0]['question']\n",
    "test_context = naive_context_assembly(documents, test_query, token_limit=4000)\n",
    "\n",
    "print(f\"‚úÖ Naive context assembled!\")\n",
    "print(f\"üìè Length: {len(test_context)} characters\")\n",
    "print(f\"üî¢ Tokens: ~{count_tokens(test_context)}\")\n",
    "print(f\"\\nüìÑ Preview:\\n{test_context[:300]}...\")\n",
    "\n",
    "# üí° HINT: Stuck? Uncomment for solution skeleton\n",
    "# %load ../src/hints/hint_naive_assembly.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Set up evaluator\n",
    "print(\"Setting up evaluation system...\")\n",
    "\n",
    "evaluator = LLMEvaluator(model, tokenizer)\n",
    "print(\"‚úÖ Evaluator ready!\")\n",
    "\n",
    "# Test on one question\n",
    "print(\"\\nüß™ Testing evaluator with one question...\")\n",
    "test_context = naive_context_assembly(documents, questions[0]['question'])\n",
    "test_answer = evaluator.generate_answer(test_context, questions[0]['question'])\n",
    "test_score = evaluator.score_answer(\n",
    "    test_answer, \n",
    "    questions[0]['ground_truth_answer']\n",
    ")\n",
    "\n",
    "print(f\"\\nQuestion: {questions[0]['question']}\")\n",
    "print(f\"Generated Answer: {test_answer}\")\n",
    "print(f\"Score: {test_score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: Evaluate naive strategy on all questions\n",
    "print(\"üî¨ Evaluating naive strategy on all questions...\")\n",
    "print(\"This may take 2-3 minutes...\\n\")\n",
    "\n",
    "naive_results = []\n",
    "\n",
    "for q in tqdm(questions, desc=\"Evaluating\"):\n",
    "    # Assemble context\n",
    "    context = naive_context_assembly(documents, q['question'], token_limit=4000)\n",
    "    \n",
    "    # Generate answer\n",
    "    answer = evaluator.generate_answer(context, q['question'])\n",
    "    \n",
    "    # Score answer\n",
    "    score = evaluator.score_answer(answer, q['ground_truth_answer'])\n",
    "    \n",
    "    naive_results.append({\n",
    "        'question_id': q['id'],\n",
    "        'question': q['question'],\n",
    "        'answer': answer,\n",
    "        'score': score,\n",
    "        'tokens_used': count_tokens(context)\n",
    "    })\n",
    "\n",
    "# Calculate metrics\n",
    "naive_accuracy = np.mean([r['score'] for r in naive_results])\n",
    "naive_tokens = np.mean([r['tokens_used'] for r in naive_results])\n",
    "\n",
    "print(f\"\\nüìä Naive Strategy Results:\")\n",
    "print(f\"   Average Accuracy: {naive_accuracy:.2%}\")\n",
    "print(f\"   Average Tokens: {naive_tokens:.0f}\")\n",
    "print(f\"   Token Efficiency: {(naive_accuracy / naive_tokens * 1000):.3f} (accuracy per 1K tokens)\")\n",
    "\n",
    "# Save for later comparison\n",
    "baseline_metrics = {\n",
    "    'strategy': 'naive',\n",
    "    'accuracy': naive_accuracy,\n",
    "    'avg_tokens': naive_tokens,\n",
    "    'all_results': naive_results\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Baseline Established!\n",
    "\n",
    "You've now measured the **naive approach** performance. This is your baseline.\n",
    "\n",
    "**Typical Results:**\n",
    "- Accuracy: 65-72%\n",
    "- Token usage: ~3800/4000\n",
    "\n",
    "## What's Wrong with Naive?\n",
    "\n",
    "1. **No relevance ranking** - Treats all documents equally\n",
    "2. **Order dependency** - First documents always included, last ones never are\n",
    "3. **Ignores the query** - Doesn't consider what's actually being asked\n",
    "4. **Wastes attention** - Model must process irrelevant info\n",
    "\n",
    "**Can we do better? Absolutely!** ‚¨áÔ∏è\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 3: Strategic Context Placement (10 minutes)\n",
    "\n",
    "## The \"Lost in the Middle\" Problem\n",
    "\n",
    "Research shows that LLMs have **positional bias**:\n",
    "- ‚úÖ **Strong recall** for information at the START of context\n",
    "- ‚úÖ **Strong recall** for information at the END of context\n",
    "- ‚ùå **Weak recall** for information in the MIDDLE\n",
    "\n",
    "This is called the **\"lost in the middle\"** phenomenon.\n",
    "\n",
    "## Three Strategies to Test\n",
    "\n",
    "### 1. Primacy Placement\n",
    "Place most relevant documents at the **beginning**\n",
    "\n",
    "### 2. Recency Placement  \n",
    "Place most relevant documents at the **end**\n",
    "\n",
    "### 3. Sandwich Placement\n",
    "Place relevant documents at **both ends**, less relevant in middle\n",
    "\n",
    "## Your Challenge\n",
    "\n",
    "Implement all three strategies and measure which performs best!\n",
    "\n",
    "**First, we need a way to rank document relevance...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 17: Implement document ranking by relevance\n",
    "def rank_documents_by_relevance(documents, query, embedder):\n",
    "    \"\"\"\n",
    "    Rank documents by semantic similarity to the query.\n",
    "    \n",
    "    Args:\n",
    "        documents: List of document dicts\n",
    "        query: Question string\n",
    "        embedder: SentenceTransformer model\n",
    "    \n",
    "    Returns:\n",
    "        List of (doc, similarity_score) tuples, sorted by score descending\n",
    "    \"\"\"\n",
    "    # Encode query\n",
    "    query_embedding = embedder.encode(query, convert_to_tensor=True)\n",
    "    \n",
    "    # Encode all documents and calculate similarity\n",
    "    ranked = []\n",
    "    for doc in documents:\n",
    "        doc_embedding = embedder.encode(doc['content'], convert_to_tensor=True)\n",
    "        similarity = calculate_similarity(query_embedding, doc_embedding)\n",
    "        ranked.append((doc, similarity.item()))\n",
    "    \n",
    "    # Sort by similarity (highest first)\n",
    "    ranked.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return ranked\n",
    "\n",
    "# Test ranking\n",
    "test_query = \"What is the lost in the middle problem?\"\n",
    "ranked_docs = rank_documents_by_relevance(documents, test_query, embedder)\n",
    "\n",
    "print(\"üìä Document Ranking for Query:\", test_query)\n",
    "print(\"\\nTop 3 most relevant:\")\n",
    "for i, (doc, score) in enumerate(ranked_docs[:3], 1):\n",
    "    print(f\"{i}. {doc['title'][:50]}... (similarity: {score:.3f})\")\n",
    "\n",
    "print(\"\\nBottom 3 least relevant:\")\n",
    "for i, (doc, score) in enumerate(ranked_docs[-3:], 1):\n",
    "    print(f\"{i}. {doc['title'][:50]}... (similarity: {score:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 18: TODO - Implement primacy placement strategy\n",
    "\n",
    "def primacy_context_assembly(documents, query, token_limit=4000, embedder=None):\n",
    "    \"\"\"\n",
    "    Primacy placement: Most relevant documents at the START.\n",
    "    \n",
    "    Args:\n",
    "        documents: List of document dicts\n",
    "        query: Question string\n",
    "        token_limit: Max tokens\n",
    "        embedder: SentenceTransformer for ranking\n",
    "    \n",
    "    Returns:\n",
    "        Assembled context string\n",
    "    \"\"\"\n",
    "    # TODO: Implement primacy strategy\n",
    "    # Step 1: Rank documents by relevance to query\n",
    "    # Step 2: Place highest-ranked docs first\n",
    "    # Step 3: Continue adding until token limit\n",
    "    # Step 4: Return formatted context\n",
    "    \n",
    "    # TODO: Your implementation here\n",
    "    # Hint: Use rank_documents_by_relevance() from above\n",
    "    # Hint: Similar to naive, but with sorted order\n",
    "    \n",
    "    pass\n",
    "\n",
    "# Test your implementation\n",
    "test_primacy_context = primacy_context_assembly(\n",
    "    documents, \n",
    "    questions[0]['question'], \n",
    "    embedder=embedder\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Primacy context assembled!\")\n",
    "print(f\"üìè Tokens: ~{count_tokens(test_primacy_context)}\")\n",
    "\n",
    "# üí° HINT: Stuck?\n",
    "# %load ../src/hints/hint_primacy.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 19: TODO - Implement recency placement strategy\n",
    "\n",
    "def recency_context_assembly(documents, query, token_limit=4000, embedder=None):\n",
    "    \"\"\"\n",
    "    Recency placement: Most relevant documents at the END.\n",
    "    \n",
    "    Args:\n",
    "        documents: List of document dicts\n",
    "        query: Question string  \n",
    "        token_limit: Max tokens\n",
    "        embedder: SentenceTransformer for ranking\n",
    "    \n",
    "    Returns:\n",
    "        Assembled context string\n",
    "    \"\"\"\n",
    "    # TODO: Implement recency strategy\n",
    "    # Step 1: Rank documents by relevance\n",
    "    # Step 2: Add documents in REVERSE rank order (least relevant first)\n",
    "    # Step 3: This puts most relevant at the end\n",
    "    # Step 4: Return formatted context\n",
    "    \n",
    "    # TODO: Your implementation here\n",
    "    # Hint: Very similar to primacy, but reverse the order!\n",
    "    \n",
    "    pass\n",
    "\n",
    "# Test\n",
    "test_recency_context = recency_context_assembly(\n",
    "    documents,\n",
    "    questions[0]['question'],\n",
    "    embedder=embedder\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Recency context assembled!\")\n",
    "print(f\"üìè Tokens: ~{count_tokens(test_recency_context)}\")\n",
    "\n",
    "# üí° HINT: Stuck?\n",
    "# %load ../src/hints/hint_recency.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 20: TODO - Implement sandwich placement strategy\n",
    "\n",
    "def sandwich_context_assembly(documents, query, token_limit=4000, embedder=None):\n",
    "    \"\"\"\n",
    "    Sandwich placement: Relevant docs at BOTH ends, less relevant in middle.\n",
    "    \n",
    "    Strategy:\n",
    "    - Top 50% of relevant docs ‚Üí split into two groups\n",
    "    - First group at START\n",
    "    - Second group at END\n",
    "    - Remaining docs in MIDDLE\n",
    "    \n",
    "    Args:\n",
    "        documents: List of document dicts\n",
    "        query: Question string\n",
    "        token_limit: Max tokens\n",
    "        embedder: SentenceTransformer for ranking\n",
    "    \n",
    "    Returns:\n",
    "        Assembled context string\n",
    "    \"\"\"\n",
    "    # TODO: Implement sandwich strategy\n",
    "    # Step 1: Rank documents by relevance\n",
    "    # Step 2: Identify top-ranked docs (most relevant)\n",
    "    # Step 3: Split top docs into two groups\n",
    "    # Step 4: Assemble: [group1] + [middle docs] + [group2]\n",
    "    # Step 5: Respect token limit throughout\n",
    "    \n",
    "    # TODO: Your implementation here\n",
    "    # Hint: This is the most complex strategy!\n",
    "    # Hint: Consider what % of top docs to sandwich (try 40%)\n",
    "    \n",
    "    pass\n",
    "\n",
    "# Test\n",
    "test_sandwich_context = sandwich_context_assembly(\n",
    "    documents,\n",
    "    questions[0]['question'],\n",
    "    embedder=embedder\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Sandwich context assembled!\")\n",
    "print(f\"üìè Tokens: ~{count_tokens(test_sandwich_context)}\")\n",
    "\n",
    "# üí° HINT: Stuck? This one is tricky!\n",
    "# %load ../src/hints/hint_sandwich.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 21: Evaluate primacy, recency, and sandwich strategies\n",
    "print(\"üî¨ Evaluating all three strategic placement approaches...\")\n",
    "print(\"This will take 5-8 minutes total...\\n\")\n",
    "\n",
    "strategies = {\n",
    "    'primacy': primacy_context_assembly,\n",
    "    'recency': recency_context_assembly,\n",
    "    'sandwich': sandwich_context_assembly\n",
    "}\n",
    "\n",
    "all_results = {'naive': baseline_metrics}  # Include baseline\n",
    "\n",
    "for strategy_name, strategy_func in strategies.items():\n",
    "    print(f\"\\nüìä Evaluating {strategy_name.upper()} strategy...\")\n",
    "    \n",
    "    results = []\n",
    "    for q in tqdm(questions, desc=f\"  {strategy_name}\"):\n",
    "        # Assemble context using this strategy\n",
    "        context = strategy_func(\n",
    "            documents, \n",
    "            q['question'], \n",
    "            token_limit=4000,\n",
    "            embedder=embedder\n",
    "        )\n",
    "        \n",
    "        # Generate and score answer\n",
    "        answer = evaluator.generate_answer(context, q['question'])\n",
    "        score = evaluator.score_answer(answer, q['ground_truth_answer'])\n",
    "        \n",
    "        results.append({\n",
    "            'question_id': q['id'],\n",
    "            'score': score,\n",
    "            'tokens_used': count_tokens(context)\n",
    "        })\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = np.mean([r['score'] for r in results])\n",
    "    avg_tokens = np.mean([r['tokens_used'] for r in results])\n",
    "    \n",
    "    all_results[strategy_name] = {\n",
    "        'strategy': strategy_name,\n",
    "        'accuracy': accuracy,\n",
    "        'avg_tokens': avg_tokens,\n",
    "        'all_results': results\n",
    "    }\n",
    "    \n",
    "    print(f\"   ‚úÖ Accuracy: {accuracy:.2%}\")\n",
    "    print(f\"   üìè Avg Tokens: {avg_tokens:.0f}\")\n",
    "\n",
    "print(\"\\nüéâ All strategies evaluated!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 22: Visualize strategy comparison\n",
    "# Create comparison dataframe\n",
    "comparison_df = pd.DataFrame([\n",
    "    {\n",
    "        'Strategy': name.capitalize(),\n",
    "        'Accuracy': metrics['accuracy'],\n",
    "        'Avg Tokens': metrics['avg_tokens'],\n",
    "        'Improvement': (metrics['accuracy'] - all_results['naive']['accuracy']) / all_results['naive']['accuracy']\n",
    "    }\n",
    "    for name, metrics in all_results.items()\n",
    "])\n",
    "\n",
    "# Sort by accuracy\n",
    "comparison_df = comparison_df.sort_values('Accuracy', ascending=False)\n",
    "\n",
    "# Display table\n",
    "print(\"üìä STRATEGY COMPARISON\\n\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "print()\n",
    "\n",
    "# Plot comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Accuracy comparison\n",
    "ax1.barh(comparison_df['Strategy'], comparison_df['Accuracy'] * 100, color='steelblue')\n",
    "ax1.set_xlabel('Accuracy (%)')\n",
    "ax1.set_title('Strategy Accuracy Comparison')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Improvement over baseline\n",
    "ax2.barh(\n",
    "    comparison_df['Strategy'][1:],  # Exclude naive (baseline)\n",
    "    comparison_df['Improvement'][1:] * 100,\n",
    "    color='green'\n",
    ")\n",
    "ax2.set_xlabel('Improvement over Baseline (%)')\n",
    "ax2.set_title('Relative Improvement')\n",
    "ax2.axvline(x=0, color='r', linestyle='--', alpha=0.5)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Phase 3 Complete!\n",
    "\n",
    "**What you discovered:**\n",
    "- Position in context matters significantly\n",
    "- Different strategies perform differently\n",
    "- Strategic placement can improve accuracy by 10-20%\n",
    "\n",
    "### Typical Results\n",
    "\n",
    "| Strategy | Expected Accuracy | Improvement |\n",
    "|----------|------------------|-------------|\n",
    "| Naive    | 65-72%           | Baseline    |\n",
    "| Primacy  | 70-77%           | +5-8%       |\n",
    "| Recency  | 75-82%           | +10-15%     |\n",
    "| Sandwich | 78-85%           | +15-20%     |\n",
    "\n",
    "**Key Insight:** The sandwich strategy usually wins by avoiding the \"lost in the middle\" problem!\n",
    "\n",
    "**But can we do even better?** Let's find out! ‚¨áÔ∏è\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
