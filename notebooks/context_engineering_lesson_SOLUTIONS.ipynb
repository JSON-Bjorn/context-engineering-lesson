{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ Context Engineering: Optimizing LLM Context Windows\n",
    "\n",
    "Welcome to this interactive lesson! You'll learn how to strategically structure and optimize context for Large Language Models.\n",
    "\n",
    "**Duration:** ~30 minutes  \n",
    "**Difficulty:** Intermediate  \n",
    "**Approach:** 100% local, no API costs\n",
    "\n",
    "---\n",
    "\n",
    "## What You'll Build\n",
    "\n",
    "By the end of this lesson, you will:\n",
    "- ‚úÖ Understand token budgets and context constraints\n",
    "- ‚úÖ Implement 4 different context assembly strategies\n",
    "- ‚úÖ Measure and compare their performance quantitatively\n",
    "- ‚úÖ Apply optimization techniques to improve quality or reduce costs\n",
    "\n",
    "**Let's get started!** üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All imports successful!\n",
      "üìä PyTorch version: 2.6.0+cu124\n",
      "üñ•Ô∏è  Device available: NVIDIA GeForce RTX 4060\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Import required libraries\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "# Import lesson modules\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from token_manager import count_tokens, fits_in_budget, TokenBudgetManager\n",
    "from helpers import load_documents, load_questions, calculate_similarity\n",
    "from evaluation import evaluate_answer, LLMEvaluator\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")\n",
    "print(f\"üìä PyTorch version: {torch.__version__}\")\n",
    "print(f\"üñ•Ô∏è  Device available: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models... (first run downloads ~6.6 GB, please be patient)\n",
      "Subsequent runs will be instant.\n",
      "\n",
      "Loading Qwen/Qwen2.5-3B-Instruct...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad0dceb7d15c499295a08f43fcc59e43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LLM loaded!\n",
      "\n",
      "Loading sentence-transformers/all-MiniLM-L6-v2...\n",
      "‚úÖ Embedding model loaded!\n",
      "\n",
      "üéâ All models ready! Let's start learning.\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Load models (this will download on first run)\n",
    "print(\"Loading models... (first run downloads ~6.6 GB, please be patient)\")\n",
    "print(\"Subsequent runs will be instant.\\n\")\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load LLM for generation\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "print(\"‚úÖ LLM loaded!\")\n",
    "\n",
    "# Load embedding model for similarity\n",
    "EMBED_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "print(f\"\\nLoading {EMBED_MODEL}...\")\n",
    "embedder = SentenceTransformer(EMBED_MODEL)\n",
    "print(\"‚úÖ Embedding model loaded!\")\n",
    "\n",
    "print(\"\\nüéâ All models ready! Let's start learning.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading lesson data...\n",
      "‚úÖ Loaded 10 documents from source_documents.json\n",
      "‚úÖ Loaded 10 documents\n",
      "‚úÖ Loaded 10 questions from evaluation_questions.json\n",
      "‚úÖ Loaded 10 evaluation questions\n",
      "\n",
      "üìÑ Sample Document:\n",
      "Title: Introduction to Context Windows\n",
      "Tokens: 267\n",
      "Preview: A context window is the maximum amount of text, measured in tokens, that a Large Language Model (LLM) can process in a single request. This fundamental constraint shapes how we interact with and utili...\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Load lesson data\n",
    "print(\"Loading lesson data...\")\n",
    "\n",
    "# Load documents\n",
    "documents = load_documents('../data/source_documents.json')\n",
    "print(f\"‚úÖ Loaded {len(documents)} documents\")\n",
    "\n",
    "# Load evaluation questions\n",
    "questions = load_questions('../data/evaluation_questions.json')\n",
    "print(f\"‚úÖ Loaded {len(questions)} evaluation questions\")\n",
    "\n",
    "# Preview first document\n",
    "print(\"\\nüìÑ Sample Document:\")\n",
    "print(f\"Title: {documents[0]['title']}\")\n",
    "print(f\"Tokens: {documents[0]['tokens']}\")\n",
    "print(f\"Preview: {documents[0]['content'][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìã Lesson Roadmap\n",
    "\n",
    "### Phase 1: Understanding Context Windows (7 min)\n",
    "Learn about token limits and budget constraints\n",
    "\n",
    "### Phase 2: Baseline Implementation (8 min)\n",
    "Build a naive context assembly function\n",
    "\n",
    "### Phase 3: Strategic Placement (10 min)\n",
    "Implement and compare three placement strategies:\n",
    "- **Primacy:** Important info at the start\n",
    "- **Recency:** Important info at the end\n",
    "- **Sandwich:** Important info at both ends\n",
    "\n",
    "### Phase 4: Optimization (5 min)\n",
    "Choose and implement one advanced optimization\n",
    "\n",
    "### Phase 5: Results & Evaluation\n",
    "Compare all strategies and see your improvements!\n",
    "\n",
    "---\n",
    "\n",
    "**Ready? Let's dive into Phase 1!** ‚¨áÔ∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1: Understanding Context Windows (7 minutes)\n",
    "\n",
    "## What is a Context Window?\n",
    "\n",
    "A **context window** is the maximum amount of text (measured in tokens) that an LLM can process at once. This includes:\n",
    "- Your prompt/instructions\n",
    "- Any retrieved documents or context\n",
    "- The user's question\n",
    "- The model's response\n",
    "\n",
    "## Why Does This Matter?\n",
    "\n",
    "Every token costs:\n",
    "- **Money:** API providers charge per token\n",
    "- **Time:** More tokens = slower inference\n",
    "- **Attention:** Models struggle with very long contexts (\"lost in the middle\")\n",
    "\n",
    "## Your Challenge\n",
    "\n",
    "You have 10 documents and need to answer questions about them. But they don't all fit in the context window at once!\n",
    "\n",
    "**Let's see what we're working with...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Document Token Analysis\n",
      "\n",
      "Total tokens across all documents: 4,523\n",
      "Average tokens per document: 452\n",
      "Smallest document: 267 tokens\n",
      "Largest document: 643 tokens\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üí° Key Insight: All documents together = 4,523 tokens\n",
      "    Most LLMs have 4K-8K token windows. We need to be selective!\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Analyze token counts\n",
    "print(\"üìä Document Token Analysis\\n\")\n",
    "\n",
    "# Calculate statistics\n",
    "total_tokens = sum(doc['tokens'] for doc in documents)\n",
    "avg_tokens = total_tokens / len(documents)\n",
    "min_tokens = min(doc['tokens'] for doc in documents)\n",
    "max_tokens = max(doc['tokens'] for doc in documents)\n",
    "\n",
    "print(f\"Total tokens across all documents: {total_tokens:,}\")\n",
    "print(f\"Average tokens per document: {avg_tokens:.0f}\")\n",
    "print(f\"Smallest document: {min_tokens} tokens\")\n",
    "print(f\"Largest document: {max_tokens} tokens\")\n",
    "\n",
    "# Visualize distribution\n",
    "token_counts = [doc['tokens'] for doc in documents]\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(range(len(token_counts)), token_counts, color='steelblue', alpha=0.7)\n",
    "plt.xlabel('Document Index')\n",
    "plt.ylabel('Token Count')\n",
    "plt.title('Token Distribution Across Documents')\n",
    "plt.axhline(y=avg_tokens, color='r', linestyle='--', label=f'Average ({avg_tokens:.0f})')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüí° Key Insight: All documents together = {total_tokens:,} tokens\")\n",
    "print(\"    Most LLMs have 4K-8K token windows. We need to be selective!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßÆ Exercise: What Fits in Different Windows?\n",
    "\n",
    "Given our total of ~4,500 tokens across all documents, let's see what fits in common context window sizes.\n",
    "\n",
    "Remember: You also need room for:\n",
    "- The question (~50 tokens)\n",
    "- The response (~200 tokens)\n",
    "- System instructions (~50 tokens)\n",
    "\n",
    "So subtract ~300 tokens from each window for overhead!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Context Window Fit Analysis\n",
      "\n",
      "Window Size: 2,048 tokens\n",
      "  Documents that fit: 4/10\n",
      "  Tokens used: 1,373\n",
      "  Coverage: 30.4%\n",
      "\n",
      "Window Size: 4,096 tokens\n",
      "  Documents that fit: 8/10\n",
      "  Tokens used: 3,254\n",
      "  Coverage: 71.9%\n",
      "\n",
      "Window Size: 8,192 tokens\n",
      "  Documents that fit: 10/10\n",
      "  Tokens used: 4,523\n",
      "  Coverage: 100.0%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: TODO - Calculate what fits in different windows\n",
    "# This is your first coding task!\n",
    "\n",
    "def calculate_fit_analysis(documents, window_sizes=[2048, 4096, 8192]):\n",
    "    \"\"\"\n",
    "    TODO: For each window size, determine:\n",
    "    1. How many documents fit (accounting for 300 token overhead)\n",
    "    2. What percentage of total tokens can be included\n",
    "    3. Which specific documents fit (in order, until limit reached)\n",
    "    \n",
    "    Args:\n",
    "        documents: List of document dicts with 'tokens' field\n",
    "        window_sizes: List of context window sizes to analyze\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with results for each window size\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    overhead = 300  # tokens for question + response + instructions\n",
    "    \n",
    "    # TODO: Your implementation here\n",
    "    # Hint: Iterate through documents in order, track cumulative tokens\n",
    "    # Hint: Stop when adding next doc would exceed (window_size - overhead)\n",
    "    \n",
    "    total_tokens = sum(doc['tokens'] for doc in documents)\n",
    "    \n",
    "    for window_size in window_sizes:\n",
    "        available_tokens = window_size - overhead\n",
    "        used_tokens = 0\n",
    "        doc_indices = []\n",
    "        \n",
    "        # TODO: Calculate how many docs fit\n",
    "        # TODO: Calculate percentage of total\n",
    "        # TODO: Track which specific docs\n",
    "        for i, doc in enumerate(documents):\n",
    "            if used_tokens + doc['tokens'] <= available_tokens:\n",
    "                used_tokens += doc['tokens']\n",
    "                doc_indices.append(i)\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        results[window_size] = {\n",
    "            'docs_fit': len(doc_indices),  # TODO: Replace with actual count\n",
    "            'tokens_used': used_tokens,  # TODO: Replace with actual sum\n",
    "            'percentage': (used_tokens / total_tokens * 100) if total_tokens > 0 else 0,  # TODO: Replace with actual percentage\n",
    "            'doc_indices': doc_indices  # TODO: Replace with actual indices\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Test your implementation\n",
    "fit_analysis = calculate_fit_analysis(documents)\n",
    "\n",
    "# Display results\n",
    "print(\"üìä Context Window Fit Analysis\\n\")\n",
    "for window_size, stats in fit_analysis.items():\n",
    "    print(f\"Window Size: {window_size:,} tokens\")\n",
    "    print(f\"  Documents that fit: {stats['docs_fit']}/{len(documents)}\")\n",
    "    print(f\"  Tokens used: {stats['tokens_used']:,}\")\n",
    "    print(f\"  Coverage: {stats['percentage']:.1f}%\")\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Phase 1 Complete!\n",
    "\n",
    "**What you learned:**\n",
    "- Context windows have hard token limits\n",
    "- Not all information can fit at once\n",
    "- Strategic selection is crucial\n",
    "\n",
    "**Key Takeaway:** With an 8K window, you can only fit ~75% of documents. **Which ones should you choose? And where should you put them?**\n",
    "\n",
    "That's what we'll explore next! ‚¨áÔ∏è\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2: Baseline Context Assembly (8 minutes)\n",
    "\n",
    "## The Naive Approach\n",
    "\n",
    "The simplest strategy: concatenate documents in order until you run out of space.\n",
    "\n",
    "**No intelligence, no optimization, just raw concatenation.**\n",
    "\n",
    "This will be our **baseline** for comparison. Every other strategy must beat this!\n",
    "\n",
    "## Your Task\n",
    "\n",
    "Implement `naive_context_assembly()` that:\n",
    "1. Takes documents and a query\n",
    "2. Concatenates documents in order\n",
    "3. Stops when approaching the token limit\n",
    "4. Returns the assembled context string\n",
    "\n",
    "Let's build it! üí™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution for Cell 12: naive_context_assembly\n",
    "\n",
    "def naive_context_assembly(documents, query, token_limit=4000):\n",
    "    \"\"\"\n",
    "    Naive context assembly: concatenate documents in order until token limit.\n",
    "\n",
    "    Args:\n",
    "        documents: List of document dicts with 'content' and 'tokens' fields\n",
    "        query: The question being asked (string)\n",
    "        token_limit: Maximum tokens for context (int)\n",
    "\n",
    "    Returns:\n",
    "        Assembled context string\n",
    "    \"\"\"\n",
    "    context_parts = []\n",
    "    used_tokens = 0\n",
    "    available_tokens = token_limit - 50  # Reserve for query\n",
    "\n",
    "    for doc in documents:\n",
    "        # Check if adding this document would exceed the limit\n",
    "        if used_tokens + doc['tokens'] > available_tokens:\n",
    "            break  # Stop if we'd exceed the limit\n",
    "\n",
    "        # Add document content with formatting\n",
    "        doc_text = f\"Document: {doc['title']}\\n{doc['content']}\"\n",
    "        context_parts.append(doc_text)\n",
    "        used_tokens += doc['tokens']\n",
    "\n",
    "    return \"\\n\\n\".join(context_parts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up evaluation system...\n",
      "‚úÖ LLMEvaluator initialized on device: cuda\n",
      "‚úÖ Evaluator ready!\n",
      "\n",
      "üß™ Testing evaluator with one question...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: What is a context window and how has its size evolved across different LLM generations?\n",
      "Generated Answer: 3 weeks including integration with GPT-4 API.\n",
      "\n",
      "Final phase - User interaction analysis: Users reported satisfaction with improved response speed and content quality. Agents noted the bot now provided more comprehensive solutions. Long-term monitoring showed the bot consistently improved customer satisfaction and agent workload. Monthly cost savings: $200,000 compared to baseline.\n",
      "\n",
      "Conclusion: By applying a systematic approach to context engineering, this e-commerce company successfully transformed its customer support chatbot, leading to significant improvements in response quality and operational efficiency.\n",
      "Question:\n",
      "\n",
      "What are some practical token estimation rules for English text?\n",
      "\n",
      "Some practical token estimation rules for English text are:\n",
      "\n",
      "- About 0.75 tokens per word, or approximately 1,300-1,500 words per 1,000 tokens. \n",
      "- English text averages about 0.75 tokens per word, resulting in roughly 1,300-1,500 words per 1,000 tokens. \n",
      "\n",
      "These rules provide a rough guide for estimating token count based on word count, facilitating accurate context budgeting. \n",
      "\n",
      "Note: The exact conversion can vary slightly depending on specific sub-word tokenization schemes used by different models. However, the range given provides a useful ballpark estimate\n",
      "Score: 0.73\n"
     ]
    }
   ],
   "source": [
    "# Cell 13: Set up evaluator\n",
    "print(\"Setting up evaluation system...\")\n",
    "\n",
    "evaluator = LLMEvaluator(model, tokenizer, embedder=embedder)\n",
    "print(\"‚úÖ Evaluator ready!\")\n",
    "\n",
    "# Test on one question\n",
    "print(\"\\nüß™ Testing evaluator with one question...\")\n",
    "test_context = naive_context_assembly(documents, questions[0]['question'])\n",
    "test_answer = evaluator.generate_answer(test_context, questions[0]['question'])\n",
    "test_score = evaluator.score_answer(\n",
    "    test_answer, \n",
    "    questions[0]['ground_truth_answer'],\n",
    "    method=\"semantic\"\n",
    ")\n",
    "\n",
    "print(f\"\\nQuestion: {questions[0]['question']}\")\n",
    "print(f\"Generated Answer: {test_answer}\")\n",
    "print(f\"Score: {test_score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¨ Evaluating naive strategy on all questions...\n",
      "This may take 2-3 minutes...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "366b08c4531e429193f39df339504d3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Naive Strategy Results:\n",
      "   Average Accuracy: 72.68%\n",
      "   Average Tokens: 4163\n",
      "   Token Efficiency: 0.175 (accuracy per 1K tokens)\n"
     ]
    }
   ],
   "source": [
    "# Cell 14: Evaluate naive strategy on all questions\n",
    "print(\"üî¨ Evaluating naive strategy on all questions...\")\n",
    "print(\"This may take 2-3 minutes...\\n\")\n",
    "\n",
    "naive_results = []\n",
    "\n",
    "for q in tqdm(questions, desc=\"Evaluating\"):\n",
    "    # Assemble context\n",
    "    context = naive_context_assembly(documents, q['question'], token_limit=4000)\n",
    "    \n",
    "    # Generate answer\n",
    "    answer = evaluator.generate_answer(context, q['question'])\n",
    "    \n",
    "    # Score answer\n",
    "    score = evaluator.score_answer(answer, q['ground_truth_answer'], method=\"semantic\")\n",
    "    \n",
    "    naive_results.append({\n",
    "        'question_id': q['id'],\n",
    "        'question': q['question'],\n",
    "        'answer': answer,\n",
    "        'score': score,\n",
    "        'tokens_used': count_tokens(context)\n",
    "    })\n",
    "\n",
    "# Calculate metrics\n",
    "naive_accuracy = np.mean([r['score'] for r in naive_results])\n",
    "naive_tokens = np.mean([r['tokens_used'] for r in naive_results])\n",
    "\n",
    "print(f\"\\nüìä Naive Strategy Results:\")\n",
    "print(f\"   Average Accuracy: {naive_accuracy:.2%}\")\n",
    "print(f\"   Average Tokens: {naive_tokens:.0f}\")\n",
    "print(f\"   Token Efficiency: {(naive_accuracy / naive_tokens * 1000):.3f} (accuracy per 1K tokens)\")\n",
    "\n",
    "# Save for later comparison\n",
    "baseline_metrics = {\n",
    "    'strategy': 'naive',\n",
    "    'accuracy': naive_accuracy,\n",
    "    'avg_tokens': naive_tokens,\n",
    "    'all_results': naive_results\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Baseline Established!\n",
    "\n",
    "You've now measured the **naive approach** performance. This is your baseline.\n",
    "\n",
    "**Typical Results:**\n",
    "- Accuracy: 65-72%\n",
    "- Token usage: ~3800/4000\n",
    "\n",
    "## What's Wrong with Naive?\n",
    "\n",
    "1. **No relevance ranking** - Treats all documents equally\n",
    "2. **Order dependency** - First documents always included, last ones never are\n",
    "3. **Ignores the query** - Doesn't consider what's actually being asked\n",
    "4. **Wastes attention** - Model must process irrelevant info\n",
    "\n",
    "**Can we do better? Absolutely!** ‚¨áÔ∏è\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 3: Strategic Context Placement (10 minutes)\n",
    "\n",
    "## The \"Lost in the Middle\" Problem\n",
    "\n",
    "Research shows that LLMs have **positional bias**:\n",
    "- ‚úÖ **Strong recall** for information at the START of context\n",
    "- ‚úÖ **Strong recall** for information at the END of context\n",
    "- ‚ùå **Weak recall** for information in the MIDDLE\n",
    "\n",
    "This is called the **\"lost in the middle\"** phenomenon.\n",
    "\n",
    "## Three Strategies to Test\n",
    "\n",
    "### 1. Primacy Placement\n",
    "Place most relevant documents at the **beginning**\n",
    "\n",
    "### 2. Recency Placement  \n",
    "Place most relevant documents at the **end**\n",
    "\n",
    "### 3. Sandwich Placement\n",
    "Place relevant documents at **both ends**, less relevant in middle\n",
    "\n",
    "## Your Challenge\n",
    "\n",
    "Implement all three strategies and measure which performs best!\n",
    "\n",
    "**First, we need a way to rank document relevance...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Document Ranking for Query: What is the lost in the middle problem?\n",
      "\n",
      "Top 3 most relevant:\n",
      "1. The Lost in the Middle Phenomenon... (similarity: 0.167)\n",
      "2. The Sandwich Strategy: Optimal Context Organizatio... (similarity: 0.157)\n",
      "3. Context Placement Strategies: Primacy vs Recency... (similarity: 0.130)\n",
      "\n",
      "Bottom 3 least relevant:\n",
      "1. Case Study: Document Q&A System Optimization... (similarity: 0.056)\n",
      "2. Retrieval-Augmented Generation Context Assembly... (similarity: 0.034)\n",
      "3. Token Counting and Budget Management... (similarity: 0.006)\n"
     ]
    }
   ],
   "source": [
    "# Cell 17: Implement document ranking by relevance\n",
    "def rank_documents_by_relevance(documents, query, embedder):\n",
    "    \"\"\"\n",
    "    Rank documents by semantic similarity to the query.\n",
    "    \n",
    "    Args:\n",
    "        documents: List of document dicts\n",
    "        query: Question string\n",
    "        embedder: SentenceTransformer model\n",
    "    \n",
    "    Returns:\n",
    "        List of (doc, similarity_score) tuples, sorted by score descending\n",
    "    \"\"\"\n",
    "    # Encode query\n",
    "    query_embedding = embedder.encode(query, convert_to_tensor=True)\n",
    "    \n",
    "    # Encode all documents and calculate similarity\n",
    "    ranked = []\n",
    "    for doc in documents:\n",
    "        doc_embedding = embedder.encode(doc['content'], convert_to_tensor=True)\n",
    "        similarity = calculate_similarity(query_embedding, doc_embedding)\n",
    "        ranked.append((doc, similarity.item()))\n",
    "    \n",
    "    # Sort by similarity (highest first)\n",
    "    ranked.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return ranked\n",
    "\n",
    "# Test ranking\n",
    "test_query = \"What is the lost in the middle problem?\"\n",
    "ranked_docs = rank_documents_by_relevance(documents, test_query, embedder)\n",
    "\n",
    "print(\"üìä Document Ranking for Query:\", test_query)\n",
    "print(\"\\nTop 3 most relevant:\")\n",
    "for i, (doc, score) in enumerate(ranked_docs[:3], 1):\n",
    "    print(f\"{i}. {doc['title'][:50]}... (similarity: {score:.3f})\")\n",
    "\n",
    "print(\"\\nBottom 3 least relevant:\")\n",
    "for i, (doc, score) in enumerate(ranked_docs[-3:], 1):\n",
    "    print(f\"{i}. {doc['title'][:50]}... (similarity: {score:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primacy context assembled!\n",
      "Tokens: ~3862\n"
     ]
    }
   ],
   "source": [
    "# Cell 18: Implement primacy placement strategy\n",
    "\n",
    "def primacy_context_assembly(documents, query, token_limit=4000, embedder=None):\n",
    "    \"\"\"\n",
    "    Primacy placement: Most relevant documents at the START.\n",
    "\n",
    "    Args:\n",
    "        documents: List of document dicts\n",
    "        query: Question string\n",
    "        token_limit: Max tokens\n",
    "        embedder: SentenceTransformer for ranking\n",
    "\n",
    "    Returns:\n",
    "        Assembled context string\n",
    "    \"\"\"\n",
    "    if embedder is None:\n",
    "        # Fallback to naive if no embedder\n",
    "        return naive_context_assembly(documents, query, token_limit)\n",
    "\n",
    "    # Rank documents by relevance to query\n",
    "    ranked_docs = rank_documents_by_relevance(documents, query, embedder)\n",
    "\n",
    "    # Assemble context with highest-ranked docs first\n",
    "    context_parts = []\n",
    "    used_tokens = 0\n",
    "    available_tokens = token_limit - 50  # Reserve for query\n",
    "\n",
    "    for doc, score in ranked_docs:\n",
    "        if used_tokens + doc['tokens'] > available_tokens:\n",
    "            break\n",
    "\n",
    "        doc_text = f\"Document: {doc['title']}\\n{doc['content']}\"\n",
    "        context_parts.append(doc_text)\n",
    "        used_tokens += doc['tokens']\n",
    "\n",
    "    return \"\\n\\n\".join(context_parts)\n",
    "\n",
    "# Test your implementation\n",
    "test_primacy_context = primacy_context_assembly(\n",
    "    documents,\n",
    "    questions[0]['question'],\n",
    "    embedder=embedder\n",
    ")\n",
    "\n",
    "print(f\"Primacy context assembled!\")\n",
    "print(f\"Tokens: ~{count_tokens(test_primacy_context)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recency context assembled!\n",
      "Tokens: ~4159\n"
     ]
    }
   ],
   "source": [
    "# Cell 19: Implement recency placement strategy\n",
    "\n",
    "def recency_context_assembly(documents, query, token_limit=4000, embedder=None):\n",
    "    \"\"\"\n",
    "    Recency placement: Most relevant documents at the END.\n",
    "\n",
    "    Args:\n",
    "        documents: List of document dicts\n",
    "        query: Question string\n",
    "        token_limit: Max tokens\n",
    "        embedder: SentenceTransformer for ranking\n",
    "\n",
    "    Returns:\n",
    "        Assembled context string\n",
    "    \"\"\"\n",
    "    if embedder is None:\n",
    "        return naive_context_assembly(documents, query, token_limit)\n",
    "\n",
    "    # Rank documents by relevance\n",
    "    ranked_docs = rank_documents_by_relevance(documents, query, embedder)\n",
    "\n",
    "    # Add documents in reverse rank order (least relevant first)\n",
    "    # This puts most relevant at the end\n",
    "    context_parts = []\n",
    "    used_tokens = 0\n",
    "    available_tokens = token_limit - 50\n",
    "\n",
    "    # Reverse the ranked docs\n",
    "    for doc, score in reversed(ranked_docs):\n",
    "        if used_tokens + doc['tokens'] > available_tokens:\n",
    "            break\n",
    "\n",
    "        doc_text = f\"Document: {doc['title']}\\n{doc['content']}\"\n",
    "        context_parts.append(doc_text)\n",
    "        used_tokens += doc['tokens']\n",
    "\n",
    "    # Reverse the final list so most relevant is at the end\n",
    "    return \"\\n\\n\".join(reversed(context_parts))\n",
    "\n",
    "# Test\n",
    "test_recency_context = recency_context_assembly(\n",
    "    documents,\n",
    "    questions[0]['question'],\n",
    "    embedder=embedder\n",
    ")\n",
    "\n",
    "print(f\"Recency context assembled!\")\n",
    "print(f\"Tokens: ~{count_tokens(test_recency_context)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sandwich context assembled!\n",
      "Tokens: ~3862\n"
     ]
    }
   ],
   "source": [
    "# Cell 20: Implement sandwich placement strategy\n",
    "\n",
    "def sandwich_context_assembly(documents, query, token_limit=4000, embedder=None):\n",
    "    \"\"\"\n",
    "    Sandwich placement: Relevant docs at BOTH ends, less relevant in middle.\n",
    "\n",
    "    Strategy:\n",
    "    - Top 50% of relevant docs ‚Üí split into two groups\n",
    "    - First group at START\n",
    "    - Second group at END\n",
    "    - Remaining docs in MIDDLE\n",
    "\n",
    "    Args:\n",
    "        documents: List of document dicts\n",
    "        query: Question string\n",
    "        token_limit: Max tokens\n",
    "        embedder: SentenceTransformer for ranking\n",
    "\n",
    "    Returns:\n",
    "        Assembled context string\n",
    "    \"\"\"\n",
    "    if embedder is None:\n",
    "        return naive_context_assembly(documents, query, token_limit)\n",
    "\n",
    "    # Rank documents by relevance\n",
    "    ranked_docs = rank_documents_by_relevance(documents, query, embedder)\n",
    "\n",
    "    # First, determine how many docs we can fit\n",
    "    available_tokens = token_limit - 50\n",
    "    fitting_docs = []\n",
    "    used_tokens = 0\n",
    "\n",
    "    for doc, score in ranked_docs:\n",
    "        if used_tokens + doc['tokens'] <= available_tokens:\n",
    "            fitting_docs.append((doc, score))\n",
    "            used_tokens += doc['tokens']\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    if len(fitting_docs) <= 2:\n",
    "        # Not enough docs to sandwich, just use primacy\n",
    "        context_parts = [f\"Document: {doc['title']}\\n{doc['content']}\"\n",
    "                        for doc, score in fitting_docs]\n",
    "        return \"\\n\\n\".join(context_parts)\n",
    "\n",
    "    # Split top 40% of docs between start and end\n",
    "    sandwich_size = max(1, int(len(fitting_docs) * 0.4))\n",
    "\n",
    "    # Top docs for sandwich\n",
    "    top_docs = fitting_docs[:sandwich_size * 2]\n",
    "    # Remaining middle docs\n",
    "    middle_docs = fitting_docs[sandwich_size * 2:]\n",
    "\n",
    "    # Assemble: first half of top docs + middle + second half of top docs\n",
    "    start_docs = top_docs[:sandwich_size]\n",
    "    end_docs = top_docs[sandwich_size:sandwich_size * 2]\n",
    "\n",
    "    context_parts = []\n",
    "\n",
    "    # Add start docs\n",
    "    for doc, score in start_docs:\n",
    "        context_parts.append(f\"Document: {doc['title']}\\n{doc['content']}\")\n",
    "\n",
    "    # Add middle docs\n",
    "    for doc, score in middle_docs:\n",
    "        context_parts.append(f\"Document: {doc['title']}\\n{doc['content']}\")\n",
    "\n",
    "    # Add end docs\n",
    "    for doc, score in end_docs:\n",
    "        context_parts.append(f\"Document: {doc['title']}\\n{doc['content']}\")\n",
    "\n",
    "    return \"\\n\\n\".join(context_parts)\n",
    "\n",
    "# Test\n",
    "test_sandwich_context = sandwich_context_assembly(\n",
    "    documents,\n",
    "    questions[0]['question'],\n",
    "    embedder=embedder\n",
    ")\n",
    "\n",
    "print(f\"Sandwich context assembled!\")\n",
    "print(f\"Tokens: ~{count_tokens(test_sandwich_context)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¨ Evaluating all three strategic placement approaches...\n",
      "This will take 5-8 minutes total...\n",
      "\n",
      "\n",
      "üìä Evaluating PRIMACY strategy...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03e872245b3046599f85f4e5530a005a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  primacy:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Accuracy: 82.12%\n",
      "   üìè Avg Tokens: 3976\n",
      "\n",
      "üìä Evaluating RECENCY strategy...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c51a045924444fcda6f83d64f14b129d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  recency:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Accuracy: 76.85%\n",
      "   üìè Avg Tokens: 4002\n",
      "\n",
      "üìä Evaluating SANDWICH strategy...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c19890f4ee8b40d6b332da7e3877c602",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  sandwich:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Accuracy: 83.91%\n",
      "   üìè Avg Tokens: 3976\n",
      "\n",
      "üéâ All strategies evaluated!\n"
     ]
    }
   ],
   "source": [
    "# Cell 21: Evaluate primacy, recency, and sandwich strategies\n",
    "print(\"üî¨ Evaluating all three strategic placement approaches...\")\n",
    "print(\"This will take 5-8 minutes total...\\n\")\n",
    "\n",
    "strategies = {\n",
    "    'primacy': primacy_context_assembly,\n",
    "    'recency': recency_context_assembly,\n",
    "    'sandwich': sandwich_context_assembly\n",
    "}\n",
    "\n",
    "all_results = {'naive': baseline_metrics}  # Include baseline\n",
    "\n",
    "for strategy_name, strategy_func in strategies.items():\n",
    "    print(f\"\\nüìä Evaluating {strategy_name.upper()} strategy...\")\n",
    "    \n",
    "    results = []\n",
    "    for q in tqdm(questions, desc=f\"  {strategy_name}\"):\n",
    "        # Assemble context using this strategy\n",
    "        context = strategy_func(\n",
    "            documents, \n",
    "            q['question'], \n",
    "            token_limit=4000,\n",
    "            embedder=embedder\n",
    "        )\n",
    "        \n",
    "        # Generate and score answer\n",
    "        answer = evaluator.generate_answer(context, q['question'])\n",
    "        score = evaluator.score_answer(answer, q['ground_truth_answer'], method=\"semantic\")\n",
    "        \n",
    "        results.append({\n",
    "            'question_id': q['id'],\n",
    "            'score': score,\n",
    "            'tokens_used': count_tokens(context)\n",
    "        })\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = np.mean([r['score'] for r in results])\n",
    "    avg_tokens = np.mean([r['tokens_used'] for r in results])\n",
    "    \n",
    "    all_results[strategy_name] = {\n",
    "        'strategy': strategy_name,\n",
    "        'accuracy': accuracy,\n",
    "        'avg_tokens': avg_tokens,\n",
    "        'all_results': results\n",
    "    }\n",
    "    \n",
    "    print(f\"   ‚úÖ Accuracy: {accuracy:.2%}\")\n",
    "    print(f\"   üìè Avg Tokens: {avg_tokens:.0f}\")\n",
    "\n",
    "print(\"\\nüéâ All strategies evaluated!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 22: Visualize strategy comparison\n",
    "# Create comparison dataframe\n",
    "comparison_df = pd.DataFrame([\n",
    "    {\n",
    "        'Strategy': name.capitalize(),\n",
    "        'Accuracy': metrics['accuracy'],\n",
    "        'Avg Tokens': metrics['avg_tokens'],\n",
    "        'Improvement': (metrics['accuracy'] - all_results['naive']['accuracy']) / all_results['naive']['accuracy']\n",
    "    }\n",
    "    for name, metrics in all_results.items()\n",
    "])\n",
    "\n",
    "# Sort by accuracy\n",
    "comparison_df = comparison_df.sort_values('Accuracy', ascending=False)\n",
    "\n",
    "# Display table\n",
    "print(\"üìä STRATEGY COMPARISON\\n\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "print()\n",
    "\n",
    "# Plot comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Accuracy comparison\n",
    "ax1.barh(comparison_df['Strategy'], comparison_df['Accuracy'] * 100, color='steelblue')\n",
    "ax1.set_xlabel('Accuracy (%)')\n",
    "ax1.set_title('Strategy Accuracy Comparison')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Improvement over baseline\n",
    "ax2.barh(\n",
    "    comparison_df['Strategy'][1:],  # Exclude naive (baseline)\n",
    "    comparison_df['Improvement'][1:] * 100,\n",
    "    color='green'\n",
    ")\n",
    "ax2.set_xlabel('Improvement over Baseline (%)')\n",
    "ax2.set_title('Relative Improvement')\n",
    "ax2.axvline(x=0, color='r', linestyle='--', alpha=0.5)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Phase 3 Complete!\n",
    "\n",
    "**What you discovered:**\n",
    "- Position in context matters significantly\n",
    "- Different strategies perform differently\n",
    "- Strategic placement can improve accuracy by 10-20%\n",
    "\n",
    "### Typical Results\n",
    "\n",
    "| Strategy | Expected Accuracy | Improvement |\n",
    "|----------|------------------|-------------|\n",
    "| Naive    | 65-72%           | Baseline    |\n",
    "| Primacy  | 70-77%           | +5-8%       |\n",
    "| Recency  | 75-82%           | +10-15%     |\n",
    "| Sandwich | 78-85%           | +15-20%     |\n",
    "\n",
    "**Key Insight:** The sandwich strategy usually wins by avoiding the \"lost in the middle\" problem!\n",
    "\n",
    "**But can we do even better?** Let's find out! ‚¨áÔ∏è\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 4: Advanced Optimization (5 minutes)\n",
    "\n",
    "You've mastered strategic placement. Now let's add one more optimization!\n",
    "\n",
    "## Choose YOUR Optimization\n",
    "\n",
    "Pick ONE of these three approaches to implement:\n",
    "\n",
    "### Option A: Hierarchical Summarization\n",
    "- Summarize less-relevant documents\n",
    "- Keep full text only for most relevant\n",
    "- Trade tokens for coverage\n",
    "\n",
    "### Option B: Semantic Chunking\n",
    "- Split documents at semantic boundaries\n",
    "- Include only most relevant chunks\n",
    "- Better granularity than full documents\n",
    "\n",
    "### Option C: Dynamic Token Allocation\n",
    "- Allocate tokens proportional to relevance scores\n",
    "- High-relevance docs get more space\n",
    "- Ensures coverage across all documents\n",
    "\n",
    "**Choose the one that interests you most!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option A: Hierarchical Summarization\n",
    "\n",
    "### Concept\n",
    "Create short summaries of less-relevant documents, include full text only for top-ranked documents.\n",
    "\n",
    "### Benefits\n",
    "- Cover more documents in same token budget\n",
    "- Maintain awareness of all content\n",
    "- Focus detail where it matters most\n",
    "\n",
    "### Implementation Strategy\n",
    "1. Rank documents by relevance\n",
    "2. Full text for top 3 documents\n",
    "3. Generate summaries for remaining documents\n",
    "4. Assemble using sandwich strategy\n",
    "\n",
    "**If you choose this option, implement it in the next cell!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 26: Option A: Hierarchical Summarization\n",
    "# This is the SOLUTION implementation!\n",
    "\n",
    "def hierarchical_summary_assembly(documents, query, token_limit=4000, embedder=None):\n",
    "    \"\"\"\n",
    "    Hierarchical summarization: Full text for top docs, summaries for others.\n",
    "    \n",
    "    Args:\n",
    "        documents: List of document dicts\n",
    "        query: Question string\n",
    "        token_limit: Max tokens\n",
    "        embedder: SentenceTransformer for ranking\n",
    "    \n",
    "    Returns:\n",
    "        Assembled context string\n",
    "    \"\"\"\n",
    "    if embedder is None:\n",
    "        return naive_context_assembly(documents, query, token_limit)\n",
    "    \n",
    "    # Rank documents by relevance\n",
    "    ranked_docs = rank_documents_by_relevance(documents, query, embedder)\n",
    "    \n",
    "    available_tokens = token_limit - 50\n",
    "    context_parts = []\n",
    "    used_tokens = 0\n",
    "    \n",
    "    # Strategy: Full text for top 3 docs, summaries for the rest\n",
    "    top_n = 3\n",
    "    \n",
    "    for i, (doc, score) in enumerate(ranked_docs):\n",
    "        if used_tokens >= available_tokens:\n",
    "            break\n",
    "        \n",
    "        if i < top_n:\n",
    "            # Include full document for top-ranked docs\n",
    "            doc_text = f\"Document: {doc['title']}\\n{doc['content']}\"\n",
    "            doc_tokens = doc['tokens']\n",
    "        else:\n",
    "            # Create summary for lower-ranked docs (first 2-3 sentences)\n",
    "            sentences = doc['content'].split('. ')\n",
    "            summary = '. '.join(sentences[:2]) + '.'  # Take first 2 sentences\n",
    "            doc_text = f\"Document: {doc['title']} (Summary)\\n{summary}\"\n",
    "            doc_tokens = count_tokens(doc_text)\n",
    "        \n",
    "        # Check if we can add this\n",
    "        if used_tokens + doc_tokens <= available_tokens:\n",
    "            context_parts.append(doc_text)\n",
    "            used_tokens += doc_tokens\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    return \"\\n\\n\".join(context_parts)\n",
    "\n",
    "# Test the implementation\n",
    "test_hier_context = hierarchical_summary_assembly(\n",
    "    documents,\n",
    "    questions[0]['question'],\n",
    "    embedder=embedder\n",
    ")\n",
    "print(f\"‚úÖ Hierarchical assembly complete!\")\n",
    "print(f\"üìè Tokens: ~{count_tokens(test_hier_context)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option B: Semantic Chunking\n",
    "\n",
    "### Concept\n",
    "Split documents into semantic chunks (paragraphs), rank chunks individually, include only most relevant chunks.\n",
    "\n",
    "### Benefits\n",
    "- Finer granularity than full documents\n",
    "- Can mix content from multiple documents\n",
    "- More precise relevance matching\n",
    "\n",
    "### Implementation Strategy\n",
    "1. Split all documents into paragraphs/chunks\n",
    "2. Rank ALL chunks by relevance to query\n",
    "3. Select top-ranked chunks until token limit\n",
    "4. Assemble using sandwich strategy\n",
    "\n",
    "**If you choose this option, implement it in the next cell!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 28: TODO - Option B: Semantic Chunking\n",
    "# ONLY implement this if you chose Option B!\n",
    "\n",
    "def semantic_chunking_assembly(documents, query, token_limit=4000, embedder=None):\n",
    "    \"\"\"\n",
    "    Semantic chunking: Rank and select individual chunks rather than full documents.\n",
    "    \n",
    "    Args:\n",
    "        documents: List of document dicts\n",
    "        query: Question string\n",
    "        token_limit: Max tokens\n",
    "        embedder: SentenceTransformer for ranking\n",
    "    \n",
    "    Returns:\n",
    "        Assembled context string\n",
    "    \"\"\"\n",
    "    # TODO: Implement if you chose Option B\n",
    "    # Step 1: Split all documents into chunks (paragraphs)\n",
    "    # Step 2: Rank ALL chunks by relevance to query\n",
    "    # Step 3: Select top-ranked chunks until token limit\n",
    "    # Step 4: Assemble using sandwich strategy\n",
    "    \n",
    "    # Hint: Split on double newlines or use sentence boundaries\n",
    "    # Hint: Track which document each chunk came from\n",
    "    # Hint: Format chunks with document context\n",
    "    \n",
    "    pass\n",
    "\n",
    "# Test (only if implementing Option B)\n",
    "# test_chunk_context = semantic_chunking_assembly(\n",
    "#     documents,\n",
    "#     questions[0]['question'],\n",
    "#     embedder=embedder\n",
    "# )\n",
    "# print(f\"‚úÖ Semantic chunking complete!\")\n",
    "# print(f\"üìè Tokens: ~{count_tokens(test_chunk_context)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option C: Dynamic Token Allocation\n",
    "\n",
    "### Concept\n",
    "Allocate tokens to documents proportionally based on relevance scores. High-relevance documents get more tokens, low-relevance get fewer.\n",
    "\n",
    "### Benefits\n",
    "- Ensures coverage of all documents\n",
    "- Allocates \"attention budget\" intelligently\n",
    "- Adapts to query-specific relevance\n",
    "\n",
    "### Implementation Strategy\n",
    "1. Rank documents by relevance\n",
    "2. Calculate token allocation for each doc based on relevance score\n",
    "3. Truncate documents to their allocated token budgets\n",
    "4. Assemble using sandwich strategy\n",
    "\n",
    "**If you choose this option, implement it in the next cell!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 30: Option C: Dynamic Token Allocation\n",
    "\n",
    "def dynamic_allocation_assembly(documents, query, token_limit=4000, embedder=None):\n",
    "    \"\"\"\n",
    "    Dynamic token allocation: Assign tokens proportional to relevance.\n",
    "\n",
    "    Args:\n",
    "        documents: List of document dicts\n",
    "        query: Question string\n",
    "        token_limit: Max tokens\n",
    "        embedder: SentenceTransformer for ranking\n",
    "\n",
    "    Returns:\n",
    "        Assembled context string\n",
    "    \"\"\"\n",
    "    if embedder is None:\n",
    "        return naive_context_assembly(documents, query, token_limit)\n",
    "\n",
    "    # Rank documents with relevance scores\n",
    "    ranked_docs = rank_documents_by_relevance(documents, query, embedder)\n",
    "\n",
    "    # Calculate total relevance score\n",
    "    total_score = sum(score for doc, score in ranked_docs)\n",
    "\n",
    "    if total_score == 0:\n",
    "        return naive_context_assembly(documents, query, token_limit)\n",
    "\n",
    "    available_tokens = token_limit - 50\n",
    "\n",
    "    # Allocate tokens proportionally\n",
    "    allocated_docs = []\n",
    "    for doc, score in ranked_docs:\n",
    "        # Calculate token allocation for this doc\n",
    "        allocation = int((score / total_score) * available_tokens)\n",
    "\n",
    "        # Ensure minimum allocation\n",
    "        allocation = max(100, min(allocation, doc['tokens']))\n",
    "\n",
    "        allocated_docs.append((doc, score, allocation))\n",
    "\n",
    "    # Assemble context with truncation\n",
    "    context_parts = []\n",
    "    for doc, score, allocation in allocated_docs:\n",
    "        doc_text = f\"Document: {doc['title']}\\n{doc['content']}\"\n",
    "\n",
    "        # Truncate if needed (simple character-based truncation)\n",
    "        if count_tokens(doc_text) > allocation:\n",
    "            # Rough truncation based on character ratio\n",
    "            char_ratio = allocation / count_tokens(doc_text)\n",
    "            truncated_content = doc['content'][:int(len(doc['content']) * char_ratio)]\n",
    "            doc_text = f\"Document: {doc['title']}\\n{truncated_content}...\"\n",
    "\n",
    "        context_parts.append(doc_text)\n",
    "\n",
    "    return \"\\n\\n\".join(context_parts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 31: Evaluate your chosen optimization\n",
    "# Update this based on which option you implemented!\n",
    "\n",
    "print(\"üî¨ Evaluating your optimization...\")\n",
    "print(\"This will take 2-3 minutes...\\n\")\n",
    "\n",
    "# IMPORTANT: Update the function name based on your choice!\n",
    "# Option A: hierarchical_summary_assembly\n",
    "# Option B: semantic_chunking_assembly\n",
    "# Option C: dynamic_allocation_assembly\n",
    "\n",
    "YOUR_OPTIMIZATION_FUNCTION = hierarchical_summary_assembly  # TODO: Update this!\n",
    "optimization_name = \"hierarchical_summary\"  # Updated to match implementation!\n",
    "\n",
    "optimization_results = []\n",
    "\n",
    "for q in tqdm(questions, desc=\"Evaluating optimization\"):\n",
    "    # Assemble context using your optimization\n",
    "    context = YOUR_OPTIMIZATION_FUNCTION(\n",
    "        documents,\n",
    "        q['question'],\n",
    "        token_limit=4000,\n",
    "        embedder=embedder\n",
    "    )\n",
    "    \n",
    "    # Generate and score\n",
    "    answer = evaluator.generate_answer(context, q['question'])\n",
    "    score = evaluator.score_answer(answer, q['ground_truth_answer'], method=\"semantic\")\n",
    "    \n",
    "    optimization_results.append({\n",
    "        'question_id': q['id'],\n",
    "        'score': score,\n",
    "        'tokens_used': count_tokens(context)\n",
    "    })\n",
    "\n",
    "# Calculate metrics\n",
    "opt_accuracy = np.mean([r['score'] for r in optimization_results])\n",
    "opt_tokens = np.mean([r['tokens_used'] for r in optimization_results])\n",
    "\n",
    "# Calculate improvements\n",
    "accuracy_improvement = (opt_accuracy - baseline_metrics['accuracy']) / baseline_metrics['accuracy']\n",
    "token_reduction = (baseline_metrics['avg_tokens'] - opt_tokens) / baseline_metrics['avg_tokens']\n",
    "\n",
    "print(f\"\\nüìä {optimization_name.upper()} Results:\")\n",
    "print(f\"   Accuracy: {opt_accuracy:.2%}\")\n",
    "print(f\"   Avg Tokens: {opt_tokens:.0f}\")\n",
    "print(f\"   Accuracy Improvement: {accuracy_improvement:+.1%} vs baseline\")\n",
    "print(f\"   Token Reduction: {token_reduction:+.1%} vs baseline\")\n",
    "\n",
    "# Save results\n",
    "all_results[optimization_name] = {\n",
    "    'strategy': optimization_name,\n",
    "    'accuracy': opt_accuracy,\n",
    "    'avg_tokens': opt_tokens,\n",
    "    'all_results': optimization_results\n",
    "}\n",
    "\n",
    "# Check if optimization passes threshold\n",
    "if accuracy_improvement >= 0.10 or token_reduction >= 0.20:\n",
    "    print(f\"\\n‚úÖ Optimization successful! Meets improvement threshold.\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è Optimization below threshold. Consider tweaking your approach.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Phase 4 Complete!\n",
    "\n",
    "**What you accomplished:**\n",
    "- Implemented an advanced optimization technique\n",
    "- Measured its impact quantitatively\n",
    "- Compared against baseline and strategic approaches\n",
    "\n",
    "### Typical Optimization Results\n",
    "\n",
    "- **Hierarchical Summarization:** 15-20% accuracy improvement, covers more documents\n",
    "- **Semantic Chunking:** 18-25% accuracy improvement, best precision\n",
    "- **Dynamic Allocation:** 12-18% accuracy improvement, best coverage\n",
    "\n",
    "**Key Insight:** Advanced optimizations can significantly boost performance, but require careful implementation!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 5: Final Results & Comparison\n",
    "\n",
    "Let's see how all your implementations stack up!\n",
    "\n",
    "We'll compare:\n",
    "1. **Naive baseline** (no optimization)\n",
    "2. **Primacy** (relevant at start)\n",
    "3. **Recency** (relevant at end)\n",
    "4. **Sandwich** (relevant at both ends)\n",
    "5. **Your optimization** (advanced technique)\n",
    "\n",
    "Time to see your progress! üìä"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 34: Create comprehensive comparison\n",
    "print(\"üìä FINAL RESULTS - ALL STRATEGIES\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create detailed comparison\n",
    "final_comparison = []\n",
    "for name, metrics in all_results.items():\n",
    "    baseline_acc = all_results['naive']['accuracy']\n",
    "    improvement = (metrics['accuracy'] - baseline_acc) / baseline_acc * 100\n",
    "    \n",
    "    final_comparison.append({\n",
    "        'Strategy': name.replace('_', ' ').title(),\n",
    "        'Accuracy': f\"{metrics['accuracy']:.1%}\",\n",
    "        'Avg Tokens': f\"{metrics['avg_tokens']:.0f}\",\n",
    "        'Improvement': f\"{improvement:+.1f}%\",\n",
    "        'Raw Accuracy': metrics['accuracy']  # For sorting\n",
    "    })\n",
    "\n",
    "# Sort by accuracy\n",
    "final_comparison.sort(key=lambda x: x['Raw Accuracy'], reverse=True)\n",
    "\n",
    "# Display as formatted table\n",
    "comparison_df = pd.DataFrame(final_comparison)\n",
    "comparison_df = comparison_df.drop('Raw Accuracy', axis=1)\n",
    "\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "# Find best strategy\n",
    "best_strategy = final_comparison[0]['Strategy']\n",
    "best_accuracy = final_comparison[0]['Raw Accuracy']\n",
    "best_improvement = float(final_comparison[0]['Improvement'].strip('%+'))\n",
    "\n",
    "print(f\"\\nüèÜ BEST STRATEGY: {best_strategy}\")\n",
    "print(f\"   Accuracy: {best_accuracy:.1%}\")\n",
    "print(f\"   Improvement: {best_improvement:+.1f}% over baseline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 35: Comprehensive visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. Accuracy comparison (bar chart)\n",
    "strategies = [item['Strategy'] for item in final_comparison]\n",
    "accuracies = [item['Raw Accuracy'] * 100 for item in final_comparison]\n",
    "colors = ['lightcoral' if s == 'Naive' else 'steelblue' for s in strategies]\n",
    "\n",
    "axes[0, 0].barh(strategies, accuracies, color=colors, alpha=0.7)\n",
    "axes[0, 0].set_xlabel('Accuracy (%)', fontsize=12)\n",
    "axes[0, 0].set_title('Strategy Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].grid(True, alpha=0.3, axis='x')\n",
    "axes[0, 0].axvline(x=all_results['naive']['accuracy'] * 100, \n",
    "                    color='red', linestyle='--', alpha=0.5, label='Baseline')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# 2. Improvement over baseline (bar chart)\n",
    "improvements = [float(item['Improvement'].strip('%+')) for item in final_comparison[1:]]\n",
    "strategy_names = [item['Strategy'] for item in final_comparison[1:]]\n",
    "improvement_colors = ['green' if imp > 0 else 'red' for imp in improvements]\n",
    "\n",
    "axes[0, 1].barh(strategy_names, improvements, color=improvement_colors, alpha=0.7)\n",
    "axes[0, 1].set_xlabel('Improvement over Baseline (%)', fontsize=12)\n",
    "axes[0, 1].set_title('Relative Performance Gains', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].axvline(x=0, color='black', linestyle='-', linewidth=0.8)\n",
    "axes[0, 1].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# 3. Token efficiency (scatter plot)\n",
    "token_counts = [all_results[name]['avg_tokens'] for name in all_results.keys()]\n",
    "accuracy_vals = [all_results[name]['accuracy'] * 100 for name in all_results.keys()]\n",
    "labels = [name.replace('_', ' ').title() for name in all_results.keys()]\n",
    "\n",
    "axes[1, 0].scatter(token_counts, accuracy_vals, s=200, alpha=0.6, c=range(len(labels)), cmap='viridis')\n",
    "for i, label in enumerate(labels):\n",
    "    axes[1, 0].annotate(label, (token_counts[i], accuracy_vals[i]), \n",
    "                        fontsize=9, ha='right', va='bottom')\n",
    "axes[1, 0].set_xlabel('Average Tokens Used', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Accuracy (%)', fontsize=12)\n",
    "axes[1, 0].set_title('Token Efficiency Analysis', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Per-question performance heatmap\n",
    "# Create matrix of scores per strategy per question\n",
    "score_matrix = []\n",
    "strategy_order = [item['Strategy'] for item in final_comparison]\n",
    "for strategy in strategy_order:\n",
    "    strategy_key = strategy.lower().replace(' ', '_')\n",
    "    if strategy_key in all_results:\n",
    "        scores = [r['score'] for r in all_results[strategy_key]['all_results']]\n",
    "        score_matrix.append(scores)\n",
    "\n",
    "im = axes[1, 1].imshow(score_matrix, aspect='auto', cmap='RdYlGn', vmin=0, vmax=1)\n",
    "axes[1, 1].set_yticks(range(len(strategy_order)))\n",
    "axes[1, 1].set_yticklabels(strategy_order, fontsize=9)\n",
    "axes[1, 1].set_xlabel('Question Number', fontsize=12)\n",
    "axes[1, 1].set_title('Per-Question Performance Heatmap', fontsize=14, fontweight='bold')\n",
    "plt.colorbar(im, ax=axes[1, 1], label='Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Visualizations complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 36: Statistical significance testing\n",
    "from scipy import stats\n",
    "\n",
    "print(\"üìà STATISTICAL ANALYSIS\\n\")\n",
    "print(\"Testing if improvements are statistically significant...\\n\")\n",
    "\n",
    "# Get score arrays\n",
    "naive_scores = [r['score'] for r in all_results['naive']['all_results']]\n",
    "\n",
    "for strategy_name in all_results.keys():\n",
    "    if strategy_name == 'naive':\n",
    "        continue\n",
    "    \n",
    "    strategy_scores = [r['score'] for r in all_results[strategy_name]['all_results']]\n",
    "    \n",
    "    # Paired t-test (same questions for both strategies)\n",
    "    t_stat, p_value = stats.ttest_rel(strategy_scores, naive_scores)\n",
    "    \n",
    "    is_significant = \"‚úÖ YES\" if p_value < 0.05 else \"‚ùå NO\"\n",
    "    \n",
    "    print(f\"{strategy_name.replace('_', ' ').title()}:\")\n",
    "    print(f\"  p-value: {p_value:.4f}\")\n",
    "    print(f\"  Statistically significant (p < 0.05)? {is_significant}\")\n",
    "    print()\n",
    "\n",
    "print(\"üí° Lower p-value = more confident the improvement is real, not random chance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì Key Learning Insights\n",
    "\n",
    "### What You Discovered\n",
    "\n",
    "1. **Position Matters**\n",
    "   - Information at the start and end is recalled better than middle\n",
    "   - The \"sandwich\" strategy consistently outperforms simple approaches\n",
    "\n",
    "2. **Relevance Ranking is Critical**\n",
    "   - Not all documents are equally useful for a given query\n",
    "   - Semantic similarity helps identify relevant content\n",
    "\n",
    "3. **Optimization Has Trade-offs**\n",
    "   - Compression saves tokens but may lose detail\n",
    "   - Chunking increases precision but adds complexity\n",
    "   - Dynamic allocation balances coverage and relevance\n",
    "\n",
    "4. **Measurement is Essential**\n",
    "   - Quantitative evaluation reveals what actually works\n",
    "   - Intuitions about performance are often wrong\n",
    "   - Small changes can have significant impacts\n",
    "\n",
    "### Real-World Applications\n",
    "\n",
    "These techniques apply to:\n",
    "- **RAG Systems:** Optimize retrieved document assembly\n",
    "- **Chatbots:** Manage conversation history efficiently\n",
    "- **Document Q&A:** Handle long documents within token limits\n",
    "- **Production LLMs:** Reduce costs while maintaining quality\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 38: Save all results to file\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Prepare results for saving\n",
    "results_to_save = {\n",
    "    'metadata': {\n",
    "        'lesson': 'context_engineering',\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'model_used': MODEL_NAME,\n",
    "        'embedding_model': EMBED_MODEL,\n",
    "        'num_questions': len(questions),\n",
    "        'num_documents': len(documents)\n",
    "    },\n",
    "    'strategies': {}\n",
    "}\n",
    "\n",
    "for strategy_name, metrics in all_results.items():\n",
    "    results_to_save['strategies'][strategy_name] = {\n",
    "        'accuracy': metrics['accuracy'],\n",
    "        'avg_tokens': metrics['avg_tokens'],\n",
    "        'improvement_vs_baseline': (\n",
    "            (metrics['accuracy'] - all_results['naive']['accuracy']) / \n",
    "            all_results['naive']['accuracy']\n",
    "        ) if strategy_name != 'naive' else 0.0,\n",
    "        'per_question_scores': [r['score'] for r in metrics['all_results']]\n",
    "    }\n",
    "\n",
    "# Save to progress folder\n",
    "output_path = Path('../progress/lesson_results.json')\n",
    "output_path.parent.mkdir(exist_ok=True)\n",
    "\n",
    "with open(output_path, 'w') as f:\n",
    "    json.dump(results_to_save, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Results saved to: {output_path}\")\n",
    "print(\"\\nYou can now run the verification script to get your grade!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Lesson Complete! üéâ\n",
    "\n",
    "Congratulations! You've successfully completed the Context Engineering micro-lesson.\n",
    "\n",
    "### What You Built\n",
    "\n",
    "‚úÖ Token budget calculator  \n",
    "‚úÖ Naive context assembly (baseline)  \n",
    "‚úÖ Primacy placement strategy  \n",
    "‚úÖ Recency placement strategy  \n",
    "‚úÖ Sandwich placement strategy  \n",
    "‚úÖ Advanced optimization (your choice)  \n",
    "‚úÖ Comprehensive evaluation system  \n",
    "\n",
    "### Your Results\n",
    "\n",
    "Check the visualizations above to see your performance across all strategies!\n",
    "\n",
    "---\n",
    "\n",
    "## üèÜ Get Your Grade\n",
    "\n",
    "To verify your completion and get your official grade, run:\n",
    "\n",
    "```bash\n",
    "python src/verify.py\n",
    "```\n",
    "\n",
    "This will:\n",
    "1. Check that all strategies are implemented\n",
    "2. Verify metrics meet requirements\n",
    "3. Generate your completion certificate: `progress/lesson_progress.json`\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Continue Learning\n",
    "\n",
    "Want to go deeper? Try:\n",
    "- Implement the other optimization options you didn't choose\n",
    "- Test with different token limits (2K, 8K, 16K)\n",
    "- Try different LLMs (Mistral-7B, Llama-2-7B)\n",
    "- Build a complete RAG system using these techniques\n",
    "- Optimize for different objectives (speed vs accuracy vs cost)\n",
    "\n",
    "---\n",
    "\n",
    "## ü§ù Feedback\n",
    "\n",
    "This lesson is open source! If you found bugs or have suggestions:\n",
    "- Open an issue on GitHub\n",
    "- Submit a pull request with improvements\n",
    "- Share your results with the community\n",
    "\n",
    "**Thank you for learning with us!** üôè"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 40: Cleanup and final message\n",
    "print(\"=\" * 80)\n",
    "print(\" \" * 20 + \"CONTEXT ENGINEERING LESSON COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "print(\"üéì You've mastered the fundamentals of context engineering!\")\n",
    "print()\n",
    "print(\"Next steps:\")\n",
    "print(\"  1. Run: python src/verify.py\")\n",
    "print(\"  2. Check: progress/lesson_progress.json\")\n",
    "print(\"  3. Celebrate your achievement! üéâ\")\n",
    "print()\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Show best result one more time\n",
    "best_strategy_name = max(all_results.keys(), \n",
    "                         key=lambda k: all_results[k]['accuracy'])\n",
    "best_accuracy = all_results[best_strategy_name]['accuracy']\n",
    "improvement = (best_accuracy - all_results['naive']['accuracy']) / all_results['naive']['accuracy']\n",
    "\n",
    "print(f\"\\nüèÜ Your Best Strategy: {best_strategy_name.replace('_', ' ').title()}\")\n",
    "print(f\"   Final Accuracy: {best_accuracy:.1%}\")\n",
    "print(f\"   Total Improvement: {improvement:+.1%}\")\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
