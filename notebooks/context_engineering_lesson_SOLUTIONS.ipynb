{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ Context Engineering: Optimizing LLM Context Windows\n",
    "\n",
    "Welcome to this interactive lesson! You'll learn how to strategically structure and optimize context for Large Language Models.\n",
    "\n",
    "**Duration:** ~30 minutes  \n",
    "**Difficulty:** Intermediate  \n",
    "**Approach:** 100% local, no API costs\n",
    "\n",
    "---\n",
    "\n",
    "## What You'll Build\n",
    "\n",
    "By the end of this lesson, you will:\n",
    "- ‚úÖ Understand token budgets and context constraints\n",
    "- ‚úÖ Implement 4 different context assembly strategies\n",
    "- ‚úÖ Measure and compare their performance quantitatively\n",
    "- ‚úÖ Apply optimization techniques to improve quality or reduce costs\n",
    "\n",
    "**Let's get started!** üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Import required libraries\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "# Import lesson modules\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from token_manager import count_tokens, fits_in_budget, TokenBudgetManager\n",
    "from helpers import load_documents, load_questions, calculate_similarity\n",
    "from evaluation import evaluate_answer, LLMEvaluator\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")\n",
    "print(f\"üìä PyTorch version: {torch.__version__}\")\n",
    "print(f\"üñ•Ô∏è  Device available: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Load models (this will download on first run)\n",
    "print(\"Loading models... (first run downloads ~6.6 GB, please be patient)\")\n",
    "print(\"Subsequent runs will be instant.\\n\")\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load LLM for generation\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "print(\"‚úÖ LLM loaded!\")\n",
    "\n",
    "# Load embedding model for similarity\n",
    "EMBED_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "print(f\"\\nLoading {EMBED_MODEL}...\")\n",
    "embedder = SentenceTransformer(EMBED_MODEL)\n",
    "print(\"‚úÖ Embedding model loaded!\")\n",
    "\n",
    "print(\"\\nüéâ All models ready! Let's start learning.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Load lesson data\n",
    "print(\"Loading lesson data...\")\n",
    "\n",
    "# Load documents\n",
    "documents = load_documents('../data/source_documents.json')\n",
    "print(f\"‚úÖ Loaded {len(documents)} documents\")\n",
    "\n",
    "# Load evaluation questions\n",
    "questions = load_questions('../data/evaluation_questions.json')\n",
    "print(f\"‚úÖ Loaded {len(questions)} evaluation questions\")\n",
    "\n",
    "# Preview first document\n",
    "print(\"\\nüìÑ Sample Document:\")\n",
    "print(f\"Title: {documents[0]['title']}\")\n",
    "print(f\"Tokens: {documents[0]['tokens']}\")\n",
    "print(f\"Preview: {documents[0]['content'][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìã Lesson Roadmap\n",
    "\n",
    "### Phase 1: Understanding Context Windows (7 min)\n",
    "Learn about token limits and budget constraints\n",
    "\n",
    "### Phase 2: Baseline Implementation (8 min)\n",
    "Build a naive context assembly function\n",
    "\n",
    "### Phase 3: Strategic Placement (10 min)\n",
    "Implement and compare three placement strategies:\n",
    "- **Primacy:** Important info at the start\n",
    "- **Recency:** Important info at the end\n",
    "- **Sandwich:** Important info at both ends\n",
    "\n",
    "### Phase 4: Optimization (5 min)\n",
    "Choose and implement one advanced optimization\n",
    "\n",
    "### Phase 5: Results & Evaluation\n",
    "Compare all strategies and see your improvements!\n",
    "\n",
    "---\n",
    "\n",
    "**Ready? Let's dive into Phase 1!** ‚¨áÔ∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1: Understanding Context Windows (7 minutes)\n",
    "\n",
    "## What is a Context Window?\n",
    "\n",
    "A **context window** is the maximum amount of text (measured in tokens) that an LLM can process at once. This includes:\n",
    "- Your prompt/instructions\n",
    "- Any retrieved documents or context\n",
    "- The user's question\n",
    "- The model's response\n",
    "\n",
    "## Why Does This Matter?\n",
    "\n",
    "Every token costs:\n",
    "- **Money:** API providers charge per token\n",
    "- **Time:** More tokens = slower inference\n",
    "- **Attention:** Models struggle with very long contexts (\"lost in the middle\")\n",
    "\n",
    "## Your Challenge\n",
    "\n",
    "You have 10 documents and need to answer questions about them. But they don't all fit in the context window at once!\n",
    "\n",
    "**Let's see what we're working with...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Analyze token counts\n",
    "print(\"üìä Document Token Analysis\\n\")\n",
    "\n",
    "# Calculate statistics\n",
    "total_tokens = sum(doc['tokens'] for doc in documents)\n",
    "avg_tokens = total_tokens / len(documents)\n",
    "min_tokens = min(doc['tokens'] for doc in documents)\n",
    "max_tokens = max(doc['tokens'] for doc in documents)\n",
    "\n",
    "print(f\"Total tokens across all documents: {total_tokens:,}\")\n",
    "print(f\"Average tokens per document: {avg_tokens:.0f}\")\n",
    "print(f\"Smallest document: {min_tokens} tokens\")\n",
    "print(f\"Largest document: {max_tokens} tokens\")\n",
    "\n",
    "# Visualize distribution\n",
    "token_counts = [doc['tokens'] for doc in documents]\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(range(len(token_counts)), token_counts, color='steelblue', alpha=0.7)\n",
    "plt.xlabel('Document Index')\n",
    "plt.ylabel('Token Count')\n",
    "plt.title('Token Distribution Across Documents')\n",
    "plt.axhline(y=avg_tokens, color='r', linestyle='--', label=f'Average ({avg_tokens:.0f})')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüí° Key Insight: All documents together = {total_tokens:,} tokens\")\n",
    "print(\"    Most LLMs have 4K-8K token windows. We need to be selective!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßÆ Exercise: What Fits in Different Windows?\n",
    "\n",
    "Given our total of ~4,500 tokens across all documents, let's see what fits in common context window sizes.\n",
    "\n",
    "Remember: You also need room for:\n",
    "- The question (~50 tokens)\n",
    "- The response (~200 tokens)\n",
    "- System instructions (~50 tokens)\n",
    "\n",
    "So subtract ~300 tokens from each window for overhead!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: TODO - Calculate what fits in different windows\n",
    "# This is your first coding task!\n",
    "\n",
    "def calculate_fit_analysis(documents, window_sizes=[2048, 4096, 8192]):\n",
    "    \"\"\"\n",
    "    TODO: For each window size, determine:\n",
    "    1. How many documents fit (accounting for 300 token overhead)\n",
    "    2. What percentage of total tokens can be included\n",
    "    3. Which specific documents fit (in order, until limit reached)\n",
    "    \n",
    "    Args:\n",
    "        documents: List of document dicts with 'tokens' field\n",
    "        window_sizes: List of context window sizes to analyze\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with results for each window size\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    overhead = 300  # tokens for question + response + instructions\n",
    "    \n",
    "    # TODO: Your implementation here\n",
    "    # Hint: Iterate through documents in order, track cumulative tokens\n",
    "    # Hint: Stop when adding next doc would exceed (window_size - overhead)\n",
    "    \n",
    "    total_tokens = sum(doc['tokens'] for doc in documents)\n",
    "    \n",
    "    for window_size in window_sizes:\n",
    "        available_tokens = window_size - overhead\n",
    "        used_tokens = 0\n",
    "        doc_indices = []\n",
    "        \n",
    "        # TODO: Calculate how many docs fit\n",
    "        # TODO: Calculate percentage of total\n",
    "        # TODO: Track which specific docs\n",
    "        for i, doc in enumerate(documents):\n",
    "            if used_tokens + doc['tokens'] <= available_tokens:\n",
    "                used_tokens += doc['tokens']\n",
    "                doc_indices.append(i)\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        results[window_size] = {\n",
    "            'docs_fit': len(doc_indices),  # TODO: Replace with actual count\n",
    "            'tokens_used': used_tokens,  # TODO: Replace with actual sum\n",
    "            'percentage': (used_tokens / total_tokens * 100) if total_tokens > 0 else 0,  # TODO: Replace with actual percentage\n",
    "            'doc_indices': doc_indices  # TODO: Replace with actual indices\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Test your implementation\n",
    "fit_analysis = calculate_fit_analysis(documents)\n",
    "\n",
    "# Display results\n",
    "print(\"üìä Context Window Fit Analysis\\n\")\n",
    "for window_size, stats in fit_analysis.items():\n",
    "    print(f\"Window Size: {window_size:,} tokens\")\n",
    "    print(f\"  Documents that fit: {stats['docs_fit']}/{len(documents)}\")\n",
    "    print(f\"  Tokens used: {stats['tokens_used']:,}\")\n",
    "    print(f\"  Coverage: {stats['percentage']:.1f}%\")\n",
    "    print()\n",
    "\n",
    "# üí° HINT: If you're stuck, uncomment the next line\n",
    "# %load ../src/hints/hint_calculate_fit.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Phase 1 Complete!\n",
    "\n",
    "**What you learned:**\n",
    "- Context windows have hard token limits\n",
    "- Not all information can fit at once\n",
    "- Strategic selection is crucial\n",
    "\n",
    "**Key Takeaway:** With an 8K window, you can only fit ~75% of documents. **Which ones should you choose? And where should you put them?**\n",
    "\n",
    "That's what we'll explore next! ‚¨áÔ∏è\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2: Baseline Context Assembly (8 minutes)\n",
    "\n",
    "## The Naive Approach\n",
    "\n",
    "The simplest strategy: concatenate documents in order until you run out of space.\n",
    "\n",
    "**No intelligence, no optimization, just raw concatenation.**\n",
    "\n",
    "This will be our **baseline** for comparison. Every other strategy must beat this!\n",
    "\n",
    "## Your Task\n",
    "\n",
    "Implement `naive_context_assembly()` that:\n",
    "1. Takes documents and a query\n",
    "2. Concatenates documents in order\n",
    "3. Stops when approaching the token limit\n",
    "4. Returns the assembled context string\n",
    "\n",
    "Let's build it! üí™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: TODO - Implement naive context assembly\n",
    "\n",
    "def naive_context_assembly(documents, query, token_limit=4000):\n",
    "    \"\"\"\n",
    "    Naive context assembly: concatenate documents in order until token limit.\n",
    "    \n",
    "    Args:\n",
    "        documents: List of document dicts with 'content' and 'tokens' fields\n",
    "        query: The question being asked (string)\n",
    "        token_limit: Maximum tokens for context (int)\n",
    "    \n",
    "    Returns:\n",
    "        Assembled context string\n",
    "    \"\"\"\n",
    "    # TODO: Implement naive concatenation\n",
    "    # Hint 1: Reserve some tokens for the query itself (~50)\n",
    "    # Hint 2: Iterate through documents in order\n",
    "    # Hint 3: Keep track of cumulative tokens\n",
    "    # Hint 4: Stop when adding next doc would exceed limit\n",
    "    # Hint 5: Format nicely with document separators\n",
    "    \n",
    "    context_parts = []\n",
    "    used_tokens = 0\n",
    "    available_tokens = token_limit - 50  # Reserve for query\n",
    "    \n",
    "    # TODO: Your implementation here\n",
    "    for i, doc in enumerate(documents):\n",
    "        if used_tokens + doc['tokens'] <= available_tokens:\n",
    "            context_parts.append(f\"--- Document {i+1} ---\\n{doc['content']}\")\n",
    "            used_tokens += doc['tokens']\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    return \"\\n\\n\".join(context_parts)\n",
    "\n",
    "\n",
    "# Test your implementation\n",
    "test_query = questions[0]['question']\n",
    "test_context = naive_context_assembly(documents, test_query, token_limit=4000)\n",
    "\n",
    "print(f\"‚úÖ Naive context assembled!\")\n",
    "print(f\"üìè Length: {len(test_context)} characters\")\n",
    "print(f\"üî¢ Tokens: ~{count_tokens(test_context)}\")\n",
    "print(f\"\\nüìÑ Preview:\\n{test_context[:300]}...\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Set up evaluator\n",
    "print(\"Setting up evaluation system...\")\n",
    "\n",
    "evaluator = LLMEvaluator(model, tokenizer)\n",
    "print(\"‚úÖ Evaluator ready!\")\n",
    "\n",
    "# Test on one question\n",
    "print(\"\\nüß™ Testing evaluator with one question...\")\n",
    "test_context = naive_context_assembly(documents, questions[0]['question'])\n",
    "test_answer = evaluator.generate_answer(test_context, questions[0]['question'])\n",
    "test_score = evaluator.score_answer(\n",
    "    test_answer, \n",
    "    questions[0]['ground_truth_answer']\n",
    ")\n",
    "\n",
    "print(f\"\\nQuestion: {questions[0]['question']}\")\n",
    "print(f\"Generated Answer: {test_answer}\")\n",
    "print(f\"Score: {test_score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: Evaluate naive strategy on all questions\n",
    "print(\"üî¨ Evaluating naive strategy on all questions...\")\n",
    "print(\"This may take 2-3 minutes...\\n\")\n",
    "\n",
    "naive_results = []\n",
    "\n",
    "for q in tqdm(questions, desc=\"Evaluating\"):\n",
    "    # Assemble context\n",
    "    context = naive_context_assembly(documents, q['question'], token_limit=4000)\n",
    "    \n",
    "    # Generate answer\n",
    "    answer = evaluator.generate_answer(context, q['question'])\n",
    "    \n",
    "    # Score answer\n",
    "    score = evaluator.score_answer(answer, q['ground_truth_answer'])\n",
    "    \n",
    "    naive_results.append({\n",
    "        'question_id': q['id'],\n",
    "        'question': q['question'],\n",
    "        'answer': answer,\n",
    "        'score': score,\n",
    "        'tokens_used': count_tokens(context)\n",
    "    })\n",
    "\n",
    "# Calculate metrics\n",
    "naive_accuracy = np.mean([r['score'] for r in naive_results])\n",
    "naive_tokens = np.mean([r['tokens_used'] for r in naive_results])\n",
    "\n",
    "print(f\"\\nüìä Naive Strategy Results:\")\n",
    "print(f\"   Average Accuracy: {naive_accuracy:.2%}\")\n",
    "print(f\"   Average Tokens: {naive_tokens:.0f}\")\n",
    "print(f\"   Token Efficiency: {(naive_accuracy / naive_tokens * 1000):.3f} (accuracy per 1K tokens)\")\n",
    "\n",
    "# Save for later comparison\n",
    "baseline_metrics = {\n",
    "    'strategy': 'naive',\n",
    "    'accuracy': naive_accuracy,\n",
    "    'avg_tokens': naive_tokens,\n",
    "    'all_results': naive_results\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Baseline Established!\n",
    "\n",
    "You've now measured the **naive approach** performance. This is your baseline.\n",
    "\n",
    "**Typical Results:**\n",
    "- Accuracy: 65-72%\n",
    "- Token usage: ~3800/4000\n",
    "\n",
    "## What's Wrong with Naive?\n",
    "\n",
    "1. **No relevance ranking** - Treats all documents equally\n",
    "2. **Order dependency** - First documents always included, last ones never are\n",
    "3. **Ignores the query** - Doesn't consider what's actually being asked\n",
    "4. **Wastes attention** - Model must process irrelevant info\n",
    "\n",
    "**Can we do better? Absolutely!** ‚¨áÔ∏è\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 3: Strategic Context Placement (10 minutes)\n",
    "\n",
    "## The \"Lost in the Middle\" Problem\n",
    "\n",
    "Research shows that LLMs have **positional bias**:\n",
    "- ‚úÖ **Strong recall** for information at the START of context\n",
    "- ‚úÖ **Strong recall** for information at the END of context\n",
    "- ‚ùå **Weak recall** for information in the MIDDLE\n",
    "\n",
    "This is called the **\"lost in the middle\"** phenomenon.\n",
    "\n",
    "## Three Strategies to Test\n",
    "\n",
    "### 1. Primacy Placement\n",
    "Place most relevant documents at the **beginning**\n",
    "\n",
    "### 2. Recency Placement  \n",
    "Place most relevant documents at the **end**\n",
    "\n",
    "### 3. Sandwich Placement\n",
    "Place relevant documents at **both ends**, less relevant in middle\n",
    "\n",
    "## Your Challenge\n",
    "\n",
    "Implement all three strategies and measure which performs best!\n",
    "\n",
    "**First, we need a way to rank document relevance...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 17: Implement document ranking by relevance\n",
    "def rank_documents_by_relevance(documents, query, embedder):\n",
    "    \"\"\"\n",
    "    Rank documents by semantic similarity to the query.\n",
    "    \n",
    "    Args:\n",
    "        documents: List of document dicts\n",
    "        query: Question string\n",
    "        embedder: SentenceTransformer model\n",
    "    \n",
    "    Returns:\n",
    "        List of (doc, similarity_score) tuples, sorted by score descending\n",
    "    \"\"\"\n",
    "    # Encode query\n",
    "    query_embedding = embedder.encode(query, convert_to_tensor=True)\n",
    "    \n",
    "    # Encode all documents and calculate similarity\n",
    "    ranked = []\n",
    "    for doc in documents:\n",
    "        doc_embedding = embedder.encode(doc['content'], convert_to_tensor=True)\n",
    "        similarity = calculate_similarity(query_embedding, doc_embedding)\n",
    "        ranked.append((doc, similarity.item()))\n",
    "    \n",
    "    # Sort by similarity (highest first)\n",
    "    ranked.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return ranked\n",
    "\n",
    "# Test ranking\n",
    "test_query = \"What is the lost in the middle problem?\"\n",
    "ranked_docs = rank_documents_by_relevance(documents, test_query, embedder)\n",
    "\n",
    "print(\"üìä Document Ranking for Query:\", test_query)\n",
    "print(\"\\nTop 3 most relevant:\")\n",
    "for i, (doc, score) in enumerate(ranked_docs[:3], 1):\n",
    "    print(f\"{i}. {doc['title'][:50]}... (similarity: {score:.3f})\")\n",
    "\n",
    "print(\"\\nBottom 3 least relevant:\")\n",
    "for i, (doc, score) in enumerate(ranked_docs[-3:], 1):\n",
    "    print(f\"{i}. {doc['title'][:50]}... (similarity: {score:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 18: TODO - Implement primacy placement strategy\n",
    "\n",
    "def primacy_context_assembly(documents, query, token_limit=4000, embedder=None):\n",
    "    \"\"\"\n",
    "    Primacy placement: Most relevant documents at the START.\n",
    "    \n",
    "    Args:\n",
    "        documents: List of document dicts\n",
    "        query: Question string\n",
    "        token_limit: Max tokens\n",
    "        embedder: SentenceTransformer for ranking\n",
    "    \n",
    "    Returns:\n",
    "        Assembled context string\n",
    "    \"\"\"\n",
    "    # TODO: Implement primacy strategy\n",
    "    # Step 1: Rank documents by relevance to query\n",
    "    # Step 2: Place highest-ranked docs first\n",
    "    # Step 3: Continue adding until token limit\n",
    "    # Step 4: Return formatted context\n",
    "    \n",
    "    # TODO: Your implementation here\n",
    "    # Hint: Use rank_documents_by_relevance() from above\n",
    "    # Hint: Similar to naive, but with sorted order\n",
    "    \n",
    "    pass\n",
    "\n",
    "# Test your implementation\n",
    "test_primacy_context = primacy_context_assembly(\n",
    "    documents, \n",
    "    questions[0]['question'], \n",
    "    embedder=embedder\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Primacy context assembled!\")\n",
    "print(f\"üìè Tokens: ~{count_tokens(test_primacy_context)}\")\n",
    "\n",
    "# üí° HINT: Stuck?\n",
    "# %load ../src/hints/hint_primacy.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 19: TODO - Implement recency placement strategy\n",
    "\n",
    "def recency_context_assembly(documents, query, token_limit=4000, embedder=None):\n",
    "    \"\"\"\n",
    "    Recency placement: Most relevant documents at the END.\n",
    "    \n",
    "    Args:\n",
    "        documents: List of document dicts\n",
    "        query: Question string  \n",
    "        token_limit: Max tokens\n",
    "        embedder: SentenceTransformer for ranking\n",
    "    \n",
    "    Returns:\n",
    "        Assembled context string\n",
    "    \"\"\"\n",
    "    # TODO: Implement recency strategy\n",
    "    # Step 1: Rank documents by relevance\n",
    "    # Step 2: Add documents in REVERSE rank order (least relevant first)\n",
    "    # Step 3: This puts most relevant at the end\n",
    "    # Step 4: Return formatted context\n",
    "    \n",
    "    # TODO: Your implementation here\n",
    "    # Hint: Very similar to primacy, but reverse the order!\n",
    "    \n",
    "    pass\n",
    "\n",
    "# Test\n",
    "test_recency_context = recency_context_assembly(\n",
    "    documents,\n",
    "    questions[0]['question'],\n",
    "    embedder=embedder\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Recency context assembled!\")\n",
    "print(f\"üìè Tokens: ~{count_tokens(test_recency_context)}\")\n",
    "\n",
    "# üí° HINT: Stuck?\n",
    "# %load ../src/hints/hint_recency.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 20: TODO - Implement sandwich placement strategy\n",
    "\n",
    "def sandwich_context_assembly(documents, query, token_limit=4000, embedder=None):\n",
    "    \"\"\"\n",
    "    Sandwich placement: Relevant docs at BOTH ends, less relevant in middle.\n",
    "    \n",
    "    Strategy:\n",
    "    - Top 50% of relevant docs ‚Üí split into two groups\n",
    "    - First group at START\n",
    "    - Second group at END\n",
    "    - Remaining docs in MIDDLE\n",
    "    \n",
    "    Args:\n",
    "        documents: List of document dicts\n",
    "        query: Question string\n",
    "        token_limit: Max tokens\n",
    "        embedder: SentenceTransformer for ranking\n",
    "    \n",
    "    Returns:\n",
    "        Assembled context string\n",
    "    \"\"\"\n",
    "    # TODO: Implement sandwich strategy\n",
    "    # Step 1: Rank documents by relevance\n",
    "    # Step 2: Identify top-ranked docs (most relevant)\n",
    "    # Step 3: Split top docs into two groups\n",
    "    # Step 4: Assemble: [group1] + [middle docs] + [group2]\n",
    "    # Step 5: Respect token limit throughout\n",
    "    \n",
    "    # TODO: Your implementation here\n",
    "    # Hint: This is the most complex strategy!\n",
    "    # Hint: Consider what % of top docs to sandwich (try 40%)\n",
    "    \n",
    "    pass\n",
    "\n",
    "# Test\n",
    "test_sandwich_context = sandwich_context_assembly(\n",
    "    documents,\n",
    "    questions[0]['question'],\n",
    "    embedder=embedder\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Sandwich context assembled!\")\n",
    "print(f\"üìè Tokens: ~{count_tokens(test_sandwich_context)}\")\n",
    "\n",
    "# üí° HINT: Stuck? This one is tricky!\n",
    "# %load ../src/hints/hint_sandwich.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 21: Evaluate primacy, recency, and sandwich strategies\n",
    "print(\"üî¨ Evaluating all three strategic placement approaches...\")\n",
    "print(\"This will take 5-8 minutes total...\\n\")\n",
    "\n",
    "strategies = {\n",
    "    'primacy': primacy_context_assembly,\n",
    "    'recency': recency_context_assembly,\n",
    "    'sandwich': sandwich_context_assembly\n",
    "}\n",
    "\n",
    "all_results = {'naive': baseline_metrics}  # Include baseline\n",
    "\n",
    "for strategy_name, strategy_func in strategies.items():\n",
    "    print(f\"\\nüìä Evaluating {strategy_name.upper()} strategy...\")\n",
    "    \n",
    "    results = []\n",
    "    for q in tqdm(questions, desc=f\"  {strategy_name}\"):\n",
    "        # Assemble context using this strategy\n",
    "        context = strategy_func(\n",
    "            documents, \n",
    "            q['question'], \n",
    "            token_limit=4000,\n",
    "            embedder=embedder\n",
    "        )\n",
    "        \n",
    "        # Generate and score answer\n",
    "        answer = evaluator.generate_answer(context, q['question'])\n",
    "        score = evaluator.score_answer(answer, q['ground_truth_answer'])\n",
    "        \n",
    "        results.append({\n",
    "            'question_id': q['id'],\n",
    "            'score': score,\n",
    "            'tokens_used': count_tokens(context)\n",
    "        })\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = np.mean([r['score'] for r in results])\n",
    "    avg_tokens = np.mean([r['tokens_used'] for r in results])\n",
    "    \n",
    "    all_results[strategy_name] = {\n",
    "        'strategy': strategy_name,\n",
    "        'accuracy': accuracy,\n",
    "        'avg_tokens': avg_tokens,\n",
    "        'all_results': results\n",
    "    }\n",
    "    \n",
    "    print(f\"   ‚úÖ Accuracy: {accuracy:.2%}\")\n",
    "    print(f\"   üìè Avg Tokens: {avg_tokens:.0f}\")\n",
    "\n",
    "print(\"\\nüéâ All strategies evaluated!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 22: Visualize strategy comparison\n",
    "# Create comparison dataframe\n",
    "comparison_df = pd.DataFrame([\n",
    "    {\n",
    "        'Strategy': name.capitalize(),\n",
    "        'Accuracy': metrics['accuracy'],\n",
    "        'Avg Tokens': metrics['avg_tokens'],\n",
    "        'Improvement': (metrics['accuracy'] - all_results['naive']['accuracy']) / all_results['naive']['accuracy']\n",
    "    }\n",
    "    for name, metrics in all_results.items()\n",
    "])\n",
    "\n",
    "# Sort by accuracy\n",
    "comparison_df = comparison_df.sort_values('Accuracy', ascending=False)\n",
    "\n",
    "# Display table\n",
    "print(\"üìä STRATEGY COMPARISON\\n\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "print()\n",
    "\n",
    "# Plot comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Accuracy comparison\n",
    "ax1.barh(comparison_df['Strategy'], comparison_df['Accuracy'] * 100, color='steelblue')\n",
    "ax1.set_xlabel('Accuracy (%)')\n",
    "ax1.set_title('Strategy Accuracy Comparison')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Improvement over baseline\n",
    "ax2.barh(\n",
    "    comparison_df['Strategy'][1:],  # Exclude naive (baseline)\n",
    "    comparison_df['Improvement'][1:] * 100,\n",
    "    color='green'\n",
    ")\n",
    "ax2.set_xlabel('Improvement over Baseline (%)')\n",
    "ax2.set_title('Relative Improvement')\n",
    "ax2.axvline(x=0, color='r', linestyle='--', alpha=0.5)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Phase 3 Complete!\n",
    "\n",
    "**What you discovered:**\n",
    "- Position in context matters significantly\n",
    "- Different strategies perform differently\n",
    "- Strategic placement can improve accuracy by 10-20%\n",
    "\n",
    "### Typical Results\n",
    "\n",
    "| Strategy | Expected Accuracy | Improvement |\n",
    "|----------|------------------|-------------|\n",
    "| Naive    | 65-72%           | Baseline    |\n",
    "| Primacy  | 70-77%           | +5-8%       |\n",
    "| Recency  | 75-82%           | +10-15%     |\n",
    "| Sandwich | 78-85%           | +15-20%     |\n",
    "\n",
    "**Key Insight:** The sandwich strategy usually wins by avoiding the \"lost in the middle\" problem!\n",
    "\n",
    "**But can we do even better?** Let's find out! ‚¨áÔ∏è\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 4: Advanced Optimization (5 minutes)\n",
    "\n",
    "You've mastered strategic placement. Now let's add one more optimization!\n",
    "\n",
    "## Choose YOUR Optimization\n",
    "\n",
    "Pick ONE of these three approaches to implement:\n",
    "\n",
    "### Option A: Hierarchical Summarization\n",
    "- Summarize less-relevant documents\n",
    "- Keep full text only for most relevant\n",
    "- Trade tokens for coverage\n",
    "\n",
    "### Option B: Semantic Chunking\n",
    "- Split documents at semantic boundaries\n",
    "- Include only most relevant chunks\n",
    "- Better granularity than full documents\n",
    "\n",
    "### Option C: Dynamic Token Allocation\n",
    "- Allocate tokens proportional to relevance scores\n",
    "- High-relevance docs get more space\n",
    "- Ensures coverage across all documents\n",
    "\n",
    "**Choose the one that interests you most!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option A: Hierarchical Summarization\n",
    "\n",
    "### Concept\n",
    "Create short summaries of less-relevant documents, include full text only for top-ranked documents.\n",
    "\n",
    "### Benefits\n",
    "- Cover more documents in same token budget\n",
    "- Maintain awareness of all content\n",
    "- Focus detail where it matters most\n",
    "\n",
    "### Implementation Strategy\n",
    "1. Rank documents by relevance\n",
    "2. Full text for top 3 documents\n",
    "3. Generate summaries for remaining documents\n",
    "4. Assemble using sandwich strategy\n",
    "\n",
    "**If you choose this option, implement it in the next cell!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 26: TODO - Option A: Hierarchical Summarization\n",
    "# ONLY implement this if you chose Option A!\n",
    "\n",
    "def hierarchical_summary_assembly(documents, query, token_limit=4000, embedder=None):\n",
    "    \"\"\"\n",
    "    Hierarchical summarization: Full text for top docs, summaries for others.\n",
    "    \n",
    "    Args:\n",
    "        documents: List of document dicts\n",
    "        query: Question string\n",
    "        token_limit: Max tokens\n",
    "        embedder: SentenceTransformer for ranking\n",
    "    \n",
    "    Returns:\n",
    "        Assembled context string\n",
    "    \"\"\"\n",
    "    # TODO: Implement if you chose Option A\n",
    "    # Step 1: Rank documents by relevance\n",
    "    # Step 2: Identify top N docs for full inclusion (try N=3)\n",
    "    # Step 3: Generate summaries for remaining docs\n",
    "    # Step 4: Assemble using sandwich strategy\n",
    "    # Step 5: Ensure within token limit\n",
    "    \n",
    "    # Hint: Use the evaluator to generate summaries\n",
    "    # Hint: Summary prompt: \"Summarize this in 2-3 sentences: {content}\"\n",
    "    \n",
    "    pass\n",
    "\n",
    "# Test (only if implementing Option A)\n",
    "# test_hier_context = hierarchical_summary_assembly(\n",
    "#     documents,\n",
    "#     questions[0]['question'],\n",
    "#     embedder=embedder\n",
    "# )\n",
    "# print(f\"‚úÖ Hierarchical assembly complete!\")\n",
    "# print(f\"üìè Tokens: ~{count_tokens(test_hier_context)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option B: Semantic Chunking\n",
    "\n",
    "### Concept\n",
    "Split documents into semantic chunks (paragraphs), rank chunks individually, include only most relevant chunks.\n",
    "\n",
    "### Benefits\n",
    "- Finer granularity than full documents\n",
    "- Can mix content from multiple documents\n",
    "- More precise relevance matching\n",
    "\n",
    "### Implementation Strategy\n",
    "1. Split all documents into paragraphs/chunks\n",
    "2. Rank ALL chunks by relevance to query\n",
    "3. Select top-ranked chunks until token limit\n",
    "4. Assemble using sandwich strategy\n",
    "\n",
    "**If you choose this option, implement it in the next cell!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 28: TODO - Option B: Semantic Chunking\n",
    "# ONLY implement this if you chose Option B!\n",
    "\n",
    "def semantic_chunking_assembly(documents, query, token_limit=4000, embedder=None):\n",
    "    \"\"\"\n",
    "    Semantic chunking: Rank and select individual chunks rather than full documents.\n",
    "    \n",
    "    Args:\n",
    "        documents: List of document dicts\n",
    "        query: Question string\n",
    "        token_limit: Max tokens\n",
    "        embedder: SentenceTransformer for ranking\n",
    "    \n",
    "    Returns:\n",
    "        Assembled context string\n",
    "    \"\"\"\n",
    "    # TODO: Implement if you chose Option B\n",
    "    # Step 1: Split all documents into chunks (paragraphs)\n",
    "    # Step 2: Rank ALL chunks by relevance to query\n",
    "    # Step 3: Select top-ranked chunks until token limit\n",
    "    # Step 4: Assemble using sandwich strategy\n",
    "    \n",
    "    # Hint: Split on double newlines or use sentence boundaries\n",
    "    # Hint: Track which document each chunk came from\n",
    "    # Hint: Format chunks with document context\n",
    "    \n",
    "    pass\n",
    "\n",
    "# Test (only if implementing Option B)\n",
    "# test_chunk_context = semantic_chunking_assembly(\n",
    "#     documents,\n",
    "#     questions[0]['question'],\n",
    "#     embedder=embedder\n",
    "# )\n",
    "# print(f\"‚úÖ Semantic chunking complete!\")\n",
    "# print(f\"üìè Tokens: ~{count_tokens(test_chunk_context)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option C: Dynamic Token Allocation\n",
    "\n",
    "### Concept\n",
    "Allocate tokens to documents proportionally based on relevance scores. High-relevance documents get more tokens, low-relevance get fewer.\n",
    "\n",
    "### Benefits\n",
    "- Ensures coverage of all documents\n",
    "- Allocates \"attention budget\" intelligently\n",
    "- Adapts to query-specific relevance\n",
    "\n",
    "### Implementation Strategy\n",
    "1. Rank documents by relevance\n",
    "2. Calculate token allocation for each doc based on relevance score\n",
    "3. Truncate documents to their allocated token budgets\n",
    "4. Assemble using sandwich strategy\n",
    "\n",
    "**If you choose this option, implement it in the next cell!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 30: TODO - Option C: Dynamic Token Allocation\n",
    "# ONLY implement this if you chose Option C!\n",
    "\n",
    "def dynamic_allocation_assembly(documents, query, token_limit=4000, embedder=None):\n",
    "    \"\"\"\n",
    "    Dynamic token allocation: Assign tokens proportional to relevance.\n",
    "    \n",
    "    Args:\n",
    "        documents: List of document dicts\n",
    "        query: Question string\n",
    "        token_limit: Max tokens\n",
    "        embedder: SentenceTransformer for ranking\n",
    "    \n",
    "    Returns:\n",
    "        Assembled context string\n",
    "    \"\"\"\n",
    "    # TODO: Implement if you chose Option C\n",
    "    # Step 1: Rank documents with relevance scores\n",
    "    # Step 2: Calculate total relevance score\n",
    "    # Step 3: Allocate tokens: (doc_score / total_score) * available_tokens\n",
    "    # Step 4: Truncate each doc to its allocation\n",
    "    # Step 5: Assemble using sandwich strategy\n",
    "    \n",
    "    # Hint: Ensure minimum allocation per doc (e.g., 100 tokens)\n",
    "    # Hint: Handle edge cases where allocation exceeds doc length\n",
    "    \n",
    "    pass\n",
    "\n",
    "# Test (only if implementing Option C)\n",
    "# test_dynamic_context = dynamic_allocation_assembly(\n",
    "#     documents,\n",
    "#     questions[0]['question'],\n",
    "#     embedder=embedder\n",
    "# )\n",
    "# print(f\"‚úÖ Dynamic allocation complete!\")\n",
    "# print(f\"üìè Tokens: ~{count_tokens(test_dynamic_context)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 31: Evaluate your chosen optimization\n",
    "# Update this based on which option you implemented!\n",
    "\n",
    "print(\"üî¨ Evaluating your optimization...\")\n",
    "print(\"This will take 2-3 minutes...\\n\")\n",
    "\n",
    "# IMPORTANT: Update the function name based on your choice!\n",
    "# Option A: hierarchical_summary_assembly\n",
    "# Option B: semantic_chunking_assembly\n",
    "# Option C: dynamic_allocation_assembly\n",
    "\n",
    "YOUR_OPTIMIZATION_FUNCTION = hierarchical_summary_assembly  # TODO: Update this!\n",
    "optimization_name = \"your_optimization\"  # TODO: Give it a name!\n",
    "\n",
    "optimization_results = []\n",
    "\n",
    "for q in tqdm(questions, desc=\"Evaluating optimization\"):\n",
    "    # Assemble context using your optimization\n",
    "    context = YOUR_OPTIMIZATION_FUNCTION(\n",
    "        documents,\n",
    "        q['question'],\n",
    "        token_limit=4000,\n",
    "        embedder=embedder\n",
    "    )\n",
    "    \n",
    "    # Generate and score\n",
    "    answer = evaluator.generate_answer(context, q['question'])\n",
    "    score = evaluator.score_answer(answer, q['ground_truth_answer'])\n",
    "    \n",
    "    optimization_results.append({\n",
    "        'question_id': q['id'],\n",
    "        'score': score,\n",
    "        'tokens_used': count_tokens(context)\n",
    "    })\n",
    "\n",
    "# Calculate metrics\n",
    "opt_accuracy = np.mean([r['score'] for r in optimization_results])\n",
    "opt_tokens = np.mean([r['tokens_used'] for r in optimization_results])\n",
    "\n",
    "# Calculate improvements\n",
    "accuracy_improvement = (opt_accuracy - baseline_metrics['accuracy']) / baseline_metrics['accuracy']\n",
    "token_reduction = (baseline_metrics['avg_tokens'] - opt_tokens) / baseline_metrics['avg_tokens']\n",
    "\n",
    "print(f\"\\nüìä {optimization_name.upper()} Results:\")\n",
    "print(f\"   Accuracy: {opt_accuracy:.2%}\")\n",
    "print(f\"   Avg Tokens: {opt_tokens:.0f}\")\n",
    "print(f\"   Accuracy Improvement: {accuracy_improvement:+.1%} vs baseline\")\n",
    "print(f\"   Token Reduction: {token_reduction:+.1%} vs baseline\")\n",
    "\n",
    "# Save results\n",
    "all_results[optimization_name] = {\n",
    "    'strategy': optimization_name,\n",
    "    'accuracy': opt_accuracy,\n",
    "    'avg_tokens': opt_tokens,\n",
    "    'all_results': optimization_results\n",
    "}\n",
    "\n",
    "# Check if optimization passes threshold\n",
    "if accuracy_improvement >= 0.10 or token_reduction >= 0.20:\n",
    "    print(f\"\\n‚úÖ Optimization successful! Meets improvement threshold.\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è Optimization below threshold. Consider tweaking your approach.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Phase 4 Complete!\n",
    "\n",
    "**What you accomplished:**\n",
    "- Implemented an advanced optimization technique\n",
    "- Measured its impact quantitatively\n",
    "- Compared against baseline and strategic approaches\n",
    "\n",
    "### Typical Optimization Results\n",
    "\n",
    "- **Hierarchical Summarization:** 15-20% accuracy improvement, covers more documents\n",
    "- **Semantic Chunking:** 18-25% accuracy improvement, best precision\n",
    "- **Dynamic Allocation:** 12-18% accuracy improvement, best coverage\n",
    "\n",
    "**Key Insight:** Advanced optimizations can significantly boost performance, but require careful implementation!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 5: Final Results & Comparison\n",
    "\n",
    "Let's see how all your implementations stack up!\n",
    "\n",
    "We'll compare:\n",
    "1. **Naive baseline** (no optimization)\n",
    "2. **Primacy** (relevant at start)\n",
    "3. **Recency** (relevant at end)\n",
    "4. **Sandwich** (relevant at both ends)\n",
    "5. **Your optimization** (advanced technique)\n",
    "\n",
    "Time to see your progress! üìä"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 34: Create comprehensive comparison\n",
    "print(\"üìä FINAL RESULTS - ALL STRATEGIES\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create detailed comparison\n",
    "final_comparison = []\n",
    "for name, metrics in all_results.items():\n",
    "    baseline_acc = all_results['naive']['accuracy']\n",
    "    improvement = (metrics['accuracy'] - baseline_acc) / baseline_acc * 100\n",
    "    \n",
    "    final_comparison.append({\n",
    "        'Strategy': name.replace('_', ' ').title(),\n",
    "        'Accuracy': f\"{metrics['accuracy']:.1%}\",\n",
    "        'Avg Tokens': f\"{metrics['avg_tokens']:.0f}\",\n",
    "        'Improvement': f\"{improvement:+.1f}%\",\n",
    "        'Raw Accuracy': metrics['accuracy']  # For sorting\n",
    "    })\n",
    "\n",
    "# Sort by accuracy\n",
    "final_comparison.sort(key=lambda x: x['Raw Accuracy'], reverse=True)\n",
    "\n",
    "# Display as formatted table\n",
    "comparison_df = pd.DataFrame(final_comparison)\n",
    "comparison_df = comparison_df.drop('Raw Accuracy', axis=1)\n",
    "\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "# Find best strategy\n",
    "best_strategy = final_comparison[0]['Strategy']\n",
    "best_accuracy = final_comparison[0]['Raw Accuracy']\n",
    "best_improvement = float(final_comparison[0]['Improvement'].strip('%+'))\n",
    "\n",
    "print(f\"\\nüèÜ BEST STRATEGY: {best_strategy}\")\n",
    "print(f\"   Accuracy: {best_accuracy:.1%}\")\n",
    "print(f\"   Improvement: {best_improvement:+.1f}% over baseline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 35: Comprehensive visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. Accuracy comparison (bar chart)\n",
    "strategies = [item['Strategy'] for item in final_comparison]\n",
    "accuracies = [item['Raw Accuracy'] * 100 for item in final_comparison]\n",
    "colors = ['lightcoral' if s == 'Naive' else 'steelblue' for s in strategies]\n",
    "\n",
    "axes[0, 0].barh(strategies, accuracies, color=colors, alpha=0.7)\n",
    "axes[0, 0].set_xlabel('Accuracy (%)', fontsize=12)\n",
    "axes[0, 0].set_title('Strategy Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].grid(True, alpha=0.3, axis='x')\n",
    "axes[0, 0].axvline(x=all_results['naive']['accuracy'] * 100, \n",
    "                    color='red', linestyle='--', alpha=0.5, label='Baseline')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# 2. Improvement over baseline (bar chart)\n",
    "improvements = [float(item['Improvement'].strip('%+')) for item in final_comparison[1:]]\n",
    "strategy_names = [item['Strategy'] for item in final_comparison[1:]]\n",
    "improvement_colors = ['green' if imp > 0 else 'red' for imp in improvements]\n",
    "\n",
    "axes[0, 1].barh(strategy_names, improvements, color=improvement_colors, alpha=0.7)\n",
    "axes[0, 1].set_xlabel('Improvement over Baseline (%)', fontsize=12)\n",
    "axes[0, 1].set_title('Relative Performance Gains', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].axvline(x=0, color='black', linestyle='-', linewidth=0.8)\n",
    "axes[0, 1].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# 3. Token efficiency (scatter plot)\n",
    "token_counts = [all_results[name]['avg_tokens'] for name in all_results.keys()]\n",
    "accuracy_vals = [all_results[name]['accuracy'] * 100 for name in all_results.keys()]\n",
    "labels = [name.replace('_', ' ').title() for name in all_results.keys()]\n",
    "\n",
    "axes[1, 0].scatter(token_counts, accuracy_vals, s=200, alpha=0.6, c=range(len(labels)), cmap='viridis')\n",
    "for i, label in enumerate(labels):\n",
    "    axes[1, 0].annotate(label, (token_counts[i], accuracy_vals[i]), \n",
    "                        fontsize=9, ha='right', va='bottom')\n",
    "axes[1, 0].set_xlabel('Average Tokens Used', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Accuracy (%)', fontsize=12)\n",
    "axes[1, 0].set_title('Token Efficiency Analysis', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Per-question performance heatmap\n",
    "# Create matrix of scores per strategy per question\n",
    "score_matrix = []\n",
    "strategy_order = [item['Strategy'] for item in final_comparison]\n",
    "for strategy in strategy_order:\n",
    "    strategy_key = strategy.lower().replace(' ', '_')\n",
    "    if strategy_key in all_results:\n",
    "        scores = [r['score'] for r in all_results[strategy_key]['all_results']]\n",
    "        score_matrix.append(scores)\n",
    "\n",
    "im = axes[1, 1].imshow(score_matrix, aspect='auto', cmap='RdYlGn', vmin=0, vmax=1)\n",
    "axes[1, 1].set_yticks(range(len(strategy_order)))\n",
    "axes[1, 1].set_yticklabels(strategy_order, fontsize=9)\n",
    "axes[1, 1].set_xlabel('Question Number', fontsize=12)\n",
    "axes[1, 1].set_title('Per-Question Performance Heatmap', fontsize=14, fontweight='bold')\n",
    "plt.colorbar(im, ax=axes[1, 1], label='Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Visualizations complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 36: Statistical significance testing\n",
    "from scipy import stats\n",
    "\n",
    "print(\"üìà STATISTICAL ANALYSIS\\n\")\n",
    "print(\"Testing if improvements are statistically significant...\\n\")\n",
    "\n",
    "# Get score arrays\n",
    "naive_scores = [r['score'] for r in all_results['naive']['all_results']]\n",
    "\n",
    "for strategy_name in all_results.keys():\n",
    "    if strategy_name == 'naive':\n",
    "        continue\n",
    "    \n",
    "    strategy_scores = [r['score'] for r in all_results[strategy_name]['all_results']]\n",
    "    \n",
    "    # Paired t-test (same questions for both strategies)\n",
    "    t_stat, p_value = stats.ttest_rel(strategy_scores, naive_scores)\n",
    "    \n",
    "    is_significant = \"‚úÖ YES\" if p_value < 0.05 else \"‚ùå NO\"\n",
    "    \n",
    "    print(f\"{strategy_name.replace('_', ' ').title()}:\")\n",
    "    print(f\"  p-value: {p_value:.4f}\")\n",
    "    print(f\"  Statistically significant (p < 0.05)? {is_significant}\")\n",
    "    print()\n",
    "\n",
    "print(\"üí° Lower p-value = more confident the improvement is real, not random chance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì Key Learning Insights\n",
    "\n",
    "### What You Discovered\n",
    "\n",
    "1. **Position Matters**\n",
    "   - Information at the start and end is recalled better than middle\n",
    "   - The \"sandwich\" strategy consistently outperforms simple approaches\n",
    "\n",
    "2. **Relevance Ranking is Critical**\n",
    "   - Not all documents are equally useful for a given query\n",
    "   - Semantic similarity helps identify relevant content\n",
    "\n",
    "3. **Optimization Has Trade-offs**\n",
    "   - Compression saves tokens but may lose detail\n",
    "   - Chunking increases precision but adds complexity\n",
    "   - Dynamic allocation balances coverage and relevance\n",
    "\n",
    "4. **Measurement is Essential**\n",
    "   - Quantitative evaluation reveals what actually works\n",
    "   - Intuitions about performance are often wrong\n",
    "   - Small changes can have significant impacts\n",
    "\n",
    "### Real-World Applications\n",
    "\n",
    "These techniques apply to:\n",
    "- **RAG Systems:** Optimize retrieved document assembly\n",
    "- **Chatbots:** Manage conversation history efficiently\n",
    "- **Document Q&A:** Handle long documents within token limits\n",
    "- **Production LLMs:** Reduce costs while maintaining quality\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 38: Save all results to file\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Prepare results for saving\n",
    "results_to_save = {\n",
    "    'metadata': {\n",
    "        'lesson': 'context_engineering',\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'model_used': MODEL_NAME,\n",
    "        'embedding_model': EMBED_MODEL,\n",
    "        'num_questions': len(questions),\n",
    "        'num_documents': len(documents)\n",
    "    },\n",
    "    'strategies': {}\n",
    "}\n",
    "\n",
    "for strategy_name, metrics in all_results.items():\n",
    "    results_to_save['strategies'][strategy_name] = {\n",
    "        'accuracy': metrics['accuracy'],\n",
    "        'avg_tokens': metrics['avg_tokens'],\n",
    "        'improvement_vs_baseline': (\n",
    "            (metrics['accuracy'] - all_results['naive']['accuracy']) / \n",
    "            all_results['naive']['accuracy']\n",
    "        ) if strategy_name != 'naive' else 0.0,\n",
    "        'per_question_scores': [r['score'] for r in metrics['all_results']]\n",
    "    }\n",
    "\n",
    "# Save to progress folder\n",
    "output_path = Path('../progress/lesson_results.json')\n",
    "output_path.parent.mkdir(exist_ok=True)\n",
    "\n",
    "with open(output_path, 'w') as f:\n",
    "    json.dump(results_to_save, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Results saved to: {output_path}\")\n",
    "print(\"\\nYou can now run the verification script to get your grade!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Lesson Complete! üéâ\n",
    "\n",
    "Congratulations! You've successfully completed the Context Engineering micro-lesson.\n",
    "\n",
    "### What You Built\n",
    "\n",
    "‚úÖ Token budget calculator  \n",
    "‚úÖ Naive context assembly (baseline)  \n",
    "‚úÖ Primacy placement strategy  \n",
    "‚úÖ Recency placement strategy  \n",
    "‚úÖ Sandwich placement strategy  \n",
    "‚úÖ Advanced optimization (your choice)  \n",
    "‚úÖ Comprehensive evaluation system  \n",
    "\n",
    "### Your Results\n",
    "\n",
    "Check the visualizations above to see your performance across all strategies!\n",
    "\n",
    "---\n",
    "\n",
    "## üèÜ Get Your Grade\n",
    "\n",
    "To verify your completion and get your official grade, run:\n",
    "\n",
    "```bash\n",
    "python src/verify.py\n",
    "```\n",
    "\n",
    "This will:\n",
    "1. Check that all strategies are implemented\n",
    "2. Verify metrics meet requirements\n",
    "3. Generate your completion certificate: `progress/lesson_progress.json`\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Continue Learning\n",
    "\n",
    "Want to go deeper? Try:\n",
    "- Implement the other optimization options you didn't choose\n",
    "- Test with different token limits (2K, 8K, 16K)\n",
    "- Try different LLMs (Mistral-7B, Llama-2-7B)\n",
    "- Build a complete RAG system using these techniques\n",
    "- Optimize for different objectives (speed vs accuracy vs cost)\n",
    "\n",
    "---\n",
    "\n",
    "## ü§ù Feedback\n",
    "\n",
    "This lesson is open source! If you found bugs or have suggestions:\n",
    "- Open an issue on GitHub\n",
    "- Submit a pull request with improvements\n",
    "- Share your results with the community\n",
    "\n",
    "**Thank you for learning with us!** üôè"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 40: Cleanup and final message\n",
    "print(\"=\" * 80)\n",
    "print(\" \" * 20 + \"CONTEXT ENGINEERING LESSON COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "print(\"üéì You've mastered the fundamentals of context engineering!\")\n",
    "print()\n",
    "print(\"Next steps:\")\n",
    "print(\"  1. Run: python src/verify.py\")\n",
    "print(\"  2. Check: progress/lesson_progress.json\")\n",
    "print(\"  3. Celebrate your achievement! üéâ\")\n",
    "print()\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Show best result one more time\n",
    "best_strategy_name = max(all_results.keys(), \n",
    "                         key=lambda k: all_results[k]['accuracy'])\n",
    "best_accuracy = all_results[best_strategy_name]['accuracy']\n",
    "improvement = (best_accuracy - all_results['naive']['accuracy']) / all_results['naive']['accuracy']\n",
    "\n",
    "print(f\"\\nüèÜ Your Best Strategy: {best_strategy_name.replace('_', ' ').title()}\")\n",
    "print(f\"   Final Accuracy: {best_accuracy:.1%}\")\n",
    "print(f\"   Total Improvement: {improvement:+.1%}\")\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
